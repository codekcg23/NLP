{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import numpy.random as random\r\n",
    "\r\n",
    "from keras.preprocessing.sequence import pad_sequences\r\n",
    "from keras.utils import to_categorical\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense, Softmax, Dropout, SimpleRNN, Embedding, TimeDistributed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "VOCAB_SIZE = 10000\r\n",
    "MAX_SEQUENCE_LENGTH = 100\r\n",
    "UNK_TOKEN = '__unk__'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data\n",
    "In this exercise, we will be doing Part-of-Speech tag prediction for a sequence of words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "POS_TAGS = {\r\n",
    "\t'NOTAG': 0,\r\n",
    "\t'#': 1,\r\n",
    "\t'$': 2,\r\n",
    "\t'&apos;&apos;': 3,\r\n",
    "\t',': 4,\r\n",
    "\t'-RRB-': 5,\r\n",
    "\t'.': 6,\r\n",
    "\t':': 7,\r\n",
    "\t'CC': 8,\r\n",
    "\t'CD': 9,\r\n",
    "\t'DT': 10,\r\n",
    "\t'EX': 11,\r\n",
    "\t'FW': 12,\r\n",
    "\t'IN': 13,\r\n",
    "\t'JJ': 14,\r\n",
    "\t'JJR': 15,\r\n",
    "\t'JJS': 16,\r\n",
    "\t'LS': 17,\r\n",
    "\t'MD': 18,\r\n",
    "\t'NN': 19,\r\n",
    "\t'NNP': 20,\r\n",
    "\t'NNPS': 21,\r\n",
    "\t'NNS': 22,\r\n",
    "\t'PDT': 23,\r\n",
    "\t'POS': 24,\r\n",
    "\t'PRP': 25,\r\n",
    "\t'PRP$': 26,\r\n",
    "\t'RB': 27,\r\n",
    "\t'RBR': 28,\r\n",
    "\t'RBS': 29,\r\n",
    "\t'RP': 30,\r\n",
    "\t'TO': 31,\r\n",
    "\t'UH': 32,\r\n",
    "\t'VB': 33,\r\n",
    "\t'VBD': 34,\r\n",
    "\t'VBG': 35,\r\n",
    "\t'VBN': 36,\r\n",
    "\t'VBP': 37,\r\n",
    "\t'VBZ': 38,\r\n",
    "\t'WDT': 39,\r\n",
    "\t'WP': 40,\r\n",
    "\t'WP$': 41,\r\n",
    "\t'WRB': 42,\r\n",
    "\t'``': 43\r\n",
    "} \r\n",
    "\r\n",
    "text = []\r\n",
    "labels = []\r\n",
    "with open('data/text.en.txt', encoding='utf-8') as fp:\r\n",
    "    for line in fp:\r\n",
    "        text.append(line.strip().split(' '))\r\n",
    "\r\n",
    "with open('data/labels.en.txt') as fp:\r\n",
    "    for line in fp:\r\n",
    "        labels.append([POS_TAGS[p] for p in line.strip().split(' ')])\r\n",
    "        \r\n",
    "assert(len(text) == len(labels))\r\n",
    "for d, l in zip(text, labels):\r\n",
    "    assert(len(d) == len(l))\r\n",
    "    \r\n",
    "data = [(d, l) for d,l in zip(text, labels)]\r\n",
    "\r\n",
    "print(\"Loaded %d samples\"%(len(data)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loaded 209772 samples\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print(data[5]) # contains the tokens followed by their corresponding POS tags"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(['&lt;', 'description', '&gt;', 'TED', 'Talk', 'Subtitles', 'and', 'Transcript', ':', 'With', 'vibrant', 'video', 'clips', 'captured', 'by', 'submarines', ',', 'David', 'Gallo', 'takes', 'us', 'to', 'some', 'of', 'Earth', '&apos;s', 'darkest', ',', 'most', 'violent', ',', 'toxic', 'and', 'beautiful', 'habitats', ',', 'the', 'valleys', 'and', 'volcanic', 'ridges', 'of', 'the', 'oceans', '&apos;', 'depths', ',', 'where', 'life', 'is', 'bizarre', ',', 'resilient', 'and', 'shockingly', 'abundant', '.', '&lt;', '/', 'description', '&gt;'], [10, 19, 13, 36, 19, 22, 8, 19, 7, 13, 14, 19, 22, 36, 13, 22, 4, 20, 20, 38, 25, 31, 10, 13, 20, 24, 16, 4, 29, 14, 4, 14, 8, 14, 22, 4, 10, 22, 8, 14, 22, 13, 10, 22, 24, 22, 4, 42, 19, 38, 14, 4, 14, 8, 27, 14, 6, 3, 20, 19, 6])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Send random seed for reproducible results\r\n",
    "random.seed(5)\r\n",
    "random.shuffle(data)\r\n",
    "\r\n",
    "total_instances = len(data)\r\n",
    "num_train_instances = int(0.7 * total_instances)\r\n",
    "num_dev_instances = int(0.1 * total_instances)\r\n",
    "num_test_instances = int(0.2 * total_instances)\r\n",
    "\r\n",
    "train = data[:num_train_instances]\r\n",
    "dev = data[num_train_instances:num_train_instances + num_dev_instances]\r\n",
    "test = data[num_train_instances + num_dev_instances:num_train_instances + num_dev_instances + num_test_instances]\r\n",
    "\r\n",
    "print(\"Train Instances: %d\"%(len(train)))\r\n",
    "print(\"Dev Instances: %d\"%(len(dev)))\r\n",
    "print(\"Test Instances: %d\"%(len(test)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Instances: 146840\n",
      "Dev Instances: 20977\n",
      "Test Instances: 41954\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_data = [d for d,_ in train]\r\n",
    "train_labels = [l for _,l in train]\r\n",
    "\r\n",
    "dev_data = [d for d,_ in dev]\r\n",
    "dev_labels = [l for _,l in dev]\r\n",
    "\r\n",
    "test_data = [d for d,_ in test]\r\n",
    "test_labels = [l for _,l in test]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Prepare vocabulary\r\n",
    "full_vocab = dict()\r\n",
    "for instance in train_data:\r\n",
    "    for token in instance:\r\n",
    "        full_vocab[token] = 1 + full_vocab.get(token, 0)\r\n",
    "\r\n",
    "# Sort vocabulary by occurence\r\n",
    "sorted_vocab = sorted(full_vocab.keys(), key=lambda word: -full_vocab[word])\r\n",
    "\r\n",
    "# Print some samples\r\n",
    "print(\"Vocabulary size: %d\"%(len(sorted_vocab)))\r\n",
    "print(\"Most frequent tokens\")\r\n",
    "for i in range(10):\r\n",
    "    print(\"\\t%s: %d\"%(sorted_vocab[i], full_vocab[sorted_vocab[i]]))\r\n",
    "print(\"Least frequent tokens\")\r\n",
    "for i in range(1,11):\r\n",
    "    print(\"\\t%s: %d\"%(sorted_vocab[-i], full_vocab[sorted_vocab[-i]]))\r\n",
    "\r\n",
    "# We can choose to limit the vocab_size here to only a portion of the original vocab,\r\n",
    "# i.e. ignore infrequent tokens to save on memory\r\n",
    "vocab_size = VOCAB_SIZE\r\n",
    "    \r\n",
    "# Create final vocab\r\n",
    "word2idx = {w: idx for idx, w in enumerate(sorted_vocab[:vocab_size])}\r\n",
    "idx2word = {idx: w for idx, w in enumerate(sorted_vocab[:vocab_size])}\r\n",
    "\r\n",
    "\r\n",
    "word2idx[UNK_TOKEN] = vocab_size\r\n",
    "idx2word[vocab_size] = UNK_TOKEN\r\n",
    "vocab_size = vocab_size + 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size: 54562\n",
      "Most frequent tokens\n",
      "\t,: 173469\n",
      "\t.: 138735\n",
      "\tthe: 109915\n",
      "\tto: 68599\n",
      "\tof: 64398\n",
      "\tand: 59512\n",
      "\ta: 57597\n",
      "\tthat: 48974\n",
      "\tI: 44784\n",
      "\tin: 40624\n",
      "Least frequent tokens\n",
      "\tincapacitates: 1\n",
      "\tbankruptcies: 1\n",
      "\tIPOs: 1\n",
      "\tes: 1\n",
      "\tDar: 1\n",
      "\tSeparate: 1\n",
      "\tsquashed: 1\n",
      "\traking: 1\n",
      "\tHeroin: 1\n",
      "\tAnticipation: 1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filter text based on vocabulary\n",
    "We will now have to replace words we do not have in the vocabulary with a special token, `__unk__` in this case"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train_data = [[t if t in word2idx else UNK_TOKEN for t in instance] for instance in train_data]\r\n",
    "dev_data = [[t if t in word2idx else UNK_TOKEN for t in instance] for instance in dev_data]\r\n",
    "test_data = [[t if t in word2idx else UNK_TOKEN for t in instance] for instance in test_data]\r\n",
    "\r\n",
    "print(\"Number of tokens filtered out as unknown:\")\r\n",
    "print(\"Train: %d/%d\"%(len([1 for instance in train_data for t in instance if t == UNK_TOKEN]), sum([len(i) for i in train_data])))\r\n",
    "print(\"Dev: %d/%d\"%(len([1 for instance in dev_data for t in instance if t == UNK_TOKEN]), sum([len(i) for i in dev_data])))\r\n",
    "print(\"Test: %d/%d\"%(len([1 for instance in test_data for t in instance if t == UNK_TOKEN]), sum([len(i) for i in test_data])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of tokens filtered out as unknown:\n",
      "Train: 120360/2988546\n",
      "Dev: 18232/426127\n",
      "Test: 36205/854125\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare data in tensor form\n",
    "Our keras models finally take tensors as input and labels, so we need to modify our data to fit this form"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "## data_to_tensor\r\n",
    "# Given a list of instances, where each instance is a list of tokens,\r\n",
    "# this function does the following:\r\n",
    "# 1: Replace each token with its corresponding index\r\n",
    "# 2: Pad sequences to MAX_SEQUENCE_LENGTH (or truncate them if longer)\r\n",
    "#       Padding is done with a unique element, in this case `vocab_size`\r\n",
    "#       The network will learn that this unique element is padding and does not\r\n",
    "#        mean anything semantically\r\n",
    "# 3: Package everything nicely as a NUM_INSTANCES x MAX_SEQUENCE_LENGTH matrix\r\n",
    "def data_to_tensor(data, pad_value=vocab_size):\r\n",
    "    # First convert from words to indices\r\n",
    "    idx_data = [[word2idx[t] for t in instance] for instance in data]\r\n",
    "    \r\n",
    "    # Create numpy representation\r\n",
    "    return pad_sequences([np.array(d) for d in idx_data], maxlen=MAX_SEQUENCE_LENGTH, value=pad_value)\r\n",
    "\r\n",
    "X_train = data_to_tensor(train_data)\r\n",
    "y_train = to_categorical(pad_sequences(train_labels, maxlen=MAX_SEQUENCE_LENGTH, value=POS_TAGS['NOTAG']))\r\n",
    "\r\n",
    "X_dev = data_to_tensor(dev_data)\r\n",
    "y_dev = to_categorical(pad_sequences(dev_labels, maxlen=MAX_SEQUENCE_LENGTH, value=POS_TAGS['NOTAG']))\r\n",
    "\r\n",
    "X_test = data_to_tensor(test_data)\r\n",
    "y_test = to_categorical(pad_sequences(test_labels, maxlen=MAX_SEQUENCE_LENGTH, value=POS_TAGS['NOTAG']))\r\n",
    "\r\n",
    "vocab_size = vocab_size + 1 # Add 1 for the padding token"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(X_train.shape)\r\n",
    "print(y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(146840, 100)\n",
      "(146840, 100, 44)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Definition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "model = Sequential()\r\n",
    "model.add(Embedding(output_dim=25, input_dim=vocab_size, input_length=MAX_SEQUENCE_LENGTH))\r\n",
    "model.add(SimpleRNN(30, return_sequences=True)) # Return output at every timestep\r\n",
    "# Output of Simple RNN is of size 100x30\r\n",
    "# We can't use a dense layer after this since that would take only a single output (summary or average)\r\n",
    "model.add(TimeDistributed(Dense(len(POS_TAGS)))) # Apply dense layer for each timestep\r\n",
    "# Output of TimeDistributed layer is 100x40 (40 being the # of POS tags)\r\n",
    "model.add(TimeDistributed(Softmax()))\r\n",
    "\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\r\n",
    "\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 25)           250050    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 100, 30)           1680      \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 100, 44)           1364      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 100, 44)           0         \n",
      "=================================================================\n",
      "Total params: 253,094\n",
      "Trainable params: 253,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_dev, y_dev))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "4589/4589 [==============================] - 165s 33ms/step - loss: 0.1839 - acc: 0.9590 - val_loss: 0.0461 - val_acc: 0.9852\n",
      "Epoch 2/10\n",
      "4589/4589 [==============================] - 145s 32ms/step - loss: 0.0402 - acc: 0.9865 - val_loss: 0.0392 - val_acc: 0.9865\n",
      "Epoch 3/10\n",
      "4589/4589 [==============================] - 157s 34ms/step - loss: 0.0361 - acc: 0.9875 - val_loss: 0.0374 - val_acc: 0.9870\n",
      "Epoch 4/10\n",
      "4589/4589 [==============================] - 152s 33ms/step - loss: 0.0344 - acc: 0.9879 - val_loss: 0.0369 - val_acc: 0.9872\n",
      "Epoch 5/10\n",
      "4589/4589 [==============================] - 153s 33ms/step - loss: 0.0334 - acc: 0.9882 - val_loss: 0.0367 - val_acc: 0.9872\n",
      "Epoch 6/10\n",
      "4589/4589 [==============================] - 156s 34ms/step - loss: 0.0327 - acc: 0.9884 - val_loss: 0.0368 - val_acc: 0.9871\n",
      "Epoch 7/10\n",
      "4589/4589 [==============================] - 144s 31ms/step - loss: 0.0321 - acc: 0.9886 - val_loss: 0.0367 - val_acc: 0.9871\n",
      "Epoch 8/10\n",
      "4589/4589 [==============================] - 145s 32ms/step - loss: 0.0317 - acc: 0.9888 - val_loss: 0.0366 - val_acc: 0.9872\n",
      "Epoch 9/10\n",
      "4589/4589 [==============================] - 141s 31ms/step - loss: 0.0313 - acc: 0.9889 - val_loss: 0.0366 - val_acc: 0.9872\n",
      "Epoch 10/10\n",
      "4589/4589 [==============================] - 146s 32ms/step - loss: 0.0310 - acc: 0.9890 - val_loss: 0.0368 - val_acc: 0.9872\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c107d8e448>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\r\n",
    "print(\"Test Set Accuracy: %0.2f%%\"%(test_acc*100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1312/1312 [==============================] - 22s 11ms/step - loss: 0.0364 - acc: 0.9874\n",
      "Test Set Accuracy: 98.74%\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "interpreter": {
   "hash": "fbbb7d2143a1d68e1cf272edf0974e702b621cb99b4ee39ce84db3bf0ffb588e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}