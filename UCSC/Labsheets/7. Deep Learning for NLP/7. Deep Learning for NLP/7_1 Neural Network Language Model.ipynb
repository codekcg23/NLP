{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "from keras.models import Sequential\r\n",
    "from keras import backend as K\r\n",
    "from keras.layers import Dense, Lambda\r\n",
    "from keras.layers import Embedding\r\n",
    "from keras.layers import LSTM\r\n",
    "from keras.utils import to_categorical\r\n",
    "from keras import regularizers"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import glob\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "\r\n",
    "NEWLINE_TOKEN = ' __newline__ '\r\n",
    "UNK_TOKEN = '__unk__'\r\n",
    "\r\n",
    "# Read and collect text\r\n",
    "train_text = \"\"\r\n",
    "dev_text = \"\"\r\n",
    "test_text = \"\"\r\n",
    "texts = [train_text, dev_text, test_text]\r\n",
    "\r\n",
    "# Try the TED data set?\r\n",
    "for text_idx, file in enumerate(['./data/shakespeare/train.txt', './data/shakespeare/val.txt', './data/ted/test.txt']):\r\n",
    "    with open(file, 'r',encoding=\"utf8\") as fp:\r\n",
    "        texts[text_idx] += NEWLINE_TOKEN.join([l.strip() for l in fp.readlines()]) + NEWLINE_TOKEN\r\n",
    "\r\n",
    "train_text, dev_text, test_text = texts\r\n",
    "\r\n",
    "print(\"Total characters:\")\r\n",
    "print(\"Train: %d\"%(len(train_text)))\r\n",
    "print(\"Dev: %d\"%(len(dev_text)))\r\n",
    "print(\"Test: %d\"%(len(test_text)))\r\n",
    "print(train_text[:100])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total characters:\n",
      "Train: 431408\n",
      "Dev: 124528\n",
      "Test: 128419\n",
      "﻿ __newline__ Project Gutenberg’s The Complete Works of William Shakespeare, by William __newline__ \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocess text\n",
    "We usually preprocess the text to remove casing information, separate out punctuations etc to make our data cleaner"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "tokens = [None, None, None]\r\n",
    "for text_idx in range(len(texts)):\r\n",
    "    tokens[text_idx] = word_tokenize(texts[text_idx].lower())\r\n",
    "\r\n",
    "train_tokens, dev_tokens, test_tokens = tokens\r\n",
    "\r\n",
    "print(\"Total tokens:\")\r\n",
    "print(\"Train: %d\"%(len(train_tokens)))\r\n",
    "print(\"Dev: %d\"%(len(dev_tokens)))\r\n",
    "print(\"Test: %d\"%(len(test_tokens)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total tokens:\n",
      "Train: 81368\n",
      "Dev: 23839\n",
      "Test: 26162\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build vocabulary"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "VOCAB_SIZE = 5000\r\n",
    "full_vocab = dict()\r\n",
    "for token in train_tokens:\r\n",
    "    full_vocab[token] = full_vocab.get(token, 0) + 1\r\n",
    "\r\n",
    "# Sort vocabulary by occurence\r\n",
    "sorted_vocab = sorted(full_vocab.keys(), key=lambda word: -full_vocab[word])\r\n",
    "\r\n",
    "# Print some samples\r\n",
    "print(\"Vocabulary size: %d\"%(len(sorted_vocab)))\r\n",
    "print(\"Most frequent tokens\")\r\n",
    "for i in range(10):\r\n",
    "    print(\"\\t%s: %d\"%(sorted_vocab[i], full_vocab[sorted_vocab[i]]))\r\n",
    "print(\"Least frequent tokens\")\r\n",
    "for i in range(1,11):\r\n",
    "    print(\"\\t%s: %d\"%(sorted_vocab[-i], full_vocab[sorted_vocab[-i]]))\r\n",
    "\r\n",
    "# Create final vocab\r\n",
    "word2idx = {w: idx for idx, w in enumerate(sorted_vocab[:VOCAB_SIZE])}\r\n",
    "idx2word = {idx: w for idx, w in enumerate(sorted_vocab[:VOCAB_SIZE])}\r\n",
    "\r\n",
    "word2idx[UNK_TOKEN] = VOCAB_SIZE # The last element is the UNK token\r\n",
    "idx2word[VOCAB_SIZE] = UNK_TOKEN\r\n",
    "VOCAB_SIZE = VOCAB_SIZE + 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocabulary size: 6647\n",
      "Most frequent tokens\n",
      "\t__newline__: 10000\n",
      "\t,: 5218\n",
      "\t.: 4361\n",
      "\tthe: 1658\n",
      "\tand: 1456\n",
      "\ti: 1414\n",
      "\tto: 1254\n",
      "\t’: 1186\n",
      "\tof: 1111\n",
      "\tmy: 906\n",
      "Least frequent tokens\n",
      "\timpossible-: 1\n",
      "\tdescried: 1\n",
      "\tapproaching: 1\n",
      "\tfull-mann: 1\n",
      "\tsixty: 1\n",
      "\tsecurity: 1\n",
      "\tassurance: 1\n",
      "\tforgo: 1\n",
      "\trenowned: 1\n",
      "\tunexecuted: 1\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Filter text based on vocabulary\n",
    "We will now have to replace words we do not have in the vocabulary with a special token, `__unk__` in this case"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "for tokens_idx in range(len(tokens)):\r\n",
    "    tokens[tokens_idx] = [t if t in word2idx else UNK_TOKEN for t in tokens[tokens_idx]]\r\n",
    "\r\n",
    "train_tokens, dev_tokens, test_tokens = tokens\r\n",
    "print(\"Number of tokens filtered out as unknown:\")\r\n",
    "print(\"Train: %d/%d\"%(len([1 for t in train_tokens if t == UNK_TOKEN]), len(train_tokens)))\r\n",
    "print(\"Dev: %d/%d\"%(len([1 for t in dev_tokens if t == UNK_TOKEN]), len(dev_tokens)))\r\n",
    "print(\"Test: %d/%d\"%(len([1 for t in test_tokens if t == UNK_TOKEN]), len(test_tokens)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of tokens filtered out as unknown:\n",
      "Train: 1647/81368\n",
      "Dev: 1938/23839\n",
      "Test: 5162/26162\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare data in tensor form\n",
    "Our keras models finally take tensors as input and labels, so we need to modify our data to fit this form"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "X_train = np.array([word2idx[t] for t in train_tokens]) # Make lists of indexes corresponding to words in each sentence and create an array of all sentences\r\n",
    "X_dev = np.array([word2idx[t] for t in dev_tokens])\r\n",
    "X_test = np.array([word2idx[t] for t in test_tokens])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our labels in this exercise are just the next words. Hence, for\n",
    "\n",
    ">   `X_train = ['hello', 'how', 'are', 'you', '?']`\n",
    "\n",
    "we will have:\n",
    "\n",
    ">    `y_train = ['how, 'are', you', '?']`\n",
    "\n",
    "Which is just `X_train[1:]`\n",
    "We will also remove the last element of `X_train`, since we do not have any label for it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def build_bag_of_words(X, context_size=1, vocab_size=VOCAB_SIZE):\r\n",
    "    num_examples = X.shape[0]-context_size  # There's no next word for the last word!\r\n",
    "    X_bow = np.zeros((num_examples, vocab_size)) # Initialize the vector\r\n",
    "    \r\n",
    "    y_bow = np.zeros((num_examples, vocab_size))\r\n",
    "    \r\n",
    "    for idx in range(num_examples):\r\n",
    "        for context_idx in range(context_size):\r\n",
    "            X_bow[idx, X[idx+context_idx]] = 1\r\n",
    "        y_bow[idx, X[idx + context_size]] = 1\r\n",
    "    \r\n",
    "    return X_bow, y_bow\r\n",
    "            \r\n",
    "def get_next_predicted_word(model, input_words, context_size=1):\r\n",
    "    if not isinstance(input_words, list):\r\n",
    "        input_words = [input_words]\r\n",
    "    input_words = input_words + [\"__unk__\"]\r\n",
    "    input_array = np.array([word2idx[w] for w in input_words])\r\n",
    "    input_bow, _ = build_bag_of_words(input_array, context_size=context_size)\r\n",
    "    scores = model.predict(input_bow)\r\n",
    "    output_word = idx2word[np.argmax(scores)]\r\n",
    "    \r\n",
    "    return output_word\r\n",
    "\r\n",
    "def get_sentence(model, start_words, context_size=1):\r\n",
    "    if not isinstance(start_words, list):\r\n",
    "        start_words = [start_words]\r\n",
    "\r\n",
    "    output = [] + start_words\r\n",
    "    while output[-1] != '__newline__' and len(output) < 100:\r\n",
    "        prev_word = get_next_predicted_word(model, output[-context_size:], context_size=context_size)\r\n",
    "        output.append(prev_word)\r\n",
    "    return \" \".join(output)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "X_train_unigram, y_train_unigram = build_bag_of_words(X_train, context_size=1)\r\n",
    "X_dev_unigram, y_dev_unigram = build_bag_of_words(X_dev, context_size=1)\r\n",
    "X_test_unigram, y_test_unigram = build_bag_of_words(X_test, context_size=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(X_train_unigram.shape)\r\n",
    "print(X_dev_unigram.shape)\r\n",
    "print(X_test_unigram.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(81367, 5001)\n",
      "(23838, 5001)\n",
      "(26161, 5001)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model = Sequential()\r\n",
    "model.add(Dense(100, input_shape=(VOCAB_SIZE,)))\r\n",
    "model.add(Dense(100, activation='relu'))\r\n",
    "model.add(Dense(100, activation='relu'))\r\n",
    "model.add(Dense(VOCAB_SIZE, activation='softmax')) # Equivalent to adding a softmax layer\r\n",
    "\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\r\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               500200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5001)              505101    \n",
      "=================================================================\n",
      "Total params: 1,025,501\n",
      "Trainable params: 1,025,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "NUM_EPOCHS = 10\r\n",
    "for epoch in range(NUM_EPOCHS):\r\n",
    "    model.fit(X_train_unigram, y_train_unigram, batch_size=128, epochs=epoch+1, initial_epoch=epoch, validation_data=(X_dev_unigram, y_dev_unigram))\r\n",
    "    print(get_sentence(model, ['i']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "636/636 [==============================] - 81s 108ms/step - loss: 6.2137 - acc: 0.1273 - val_loss: 5.1398 - val_acc: 0.1376\n",
      "i . __newline__\n",
      "Epoch 2/2\n",
      "636/636 [==============================] - 74s 105ms/step - loss: 5.0984 - acc: 0.1937 - val_loss: 4.9158 - val_acc: 0.1630\n",
      "i have the __unk__ , __newline__\n",
      "Epoch 3/3\n",
      "636/636 [==============================] - 68s 95ms/step - loss: 4.8558 - acc: 0.2089 - val_loss: 4.8446 - val_acc: 0.1675\n",
      "i have , __newline__\n",
      "Epoch 4/4\n",
      "636/636 [==============================] - ETA: 0s - loss: 4.7021 - acc: 0.2199"
     ]
    },
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 455. MiB for an array with shape (23838, 5001) and data type float32",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3f42c10d3c7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_unigram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_unigram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_dev_unigram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_dev_unigram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1128\u001b[0m                 \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1129\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1130\u001b[1;33m                 steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1131\u001b[0m           val_logs = self.evaluate(\n\u001b[0;32m   1132\u001b[0m               \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[0;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1112\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    261\u001b[0m                **kwargs):\n\u001b[0;32m    262\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[0;32m    265\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m   \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1009\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor_v2_with_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mscipy_sparse\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mscipy_sparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1403\u001b[0m   \"\"\"\n\u001b[0;32m   1404\u001b[0m   return convert_to_tensor_v2(\n\u001b[1;32m-> 1405\u001b[1;33m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[0;32m   1406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1413\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1415\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1540\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1542\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    263\u001b[0m   \"\"\"\n\u001b[0;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[1;32m--> 265\u001b[1;33m                         allow_broadcast=True)\n\u001b[0m\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    274\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m   \u001b[1;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m   \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 455. MiB for an array with shape (23838, 5001) and data type float32"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(get_sentence(model, ['think']))\r\n",
    "print(get_sentence(model, ['well']))\r\n",
    "print(get_sentence(model, ['i']))\r\n",
    "print(get_sentence(model, ['who']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "think __newline__\n",
      "well . __newline__\n",
      "i __unk__ , __newline__\n",
      "who , __newline__\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Add context\n",
    "The above data uses only _one_ previous word as context, but we can change our data to include more words"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "X_train_bigram, y_train_bigram = build_bag_of_words(X_train, context_size=2)\r\n",
    "X_dev_bigram, y_dev_bigram = build_bag_of_words(X_dev, context_size=2)\r\n",
    "X_test_bigram, y_test_bigram = build_bag_of_words(X_test, context_size=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "print(X_train_bigram.shape)\r\n",
    "print(y_train_bigram.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(81366, 5001)\n",
      "(81366, 5001)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "model_bigram = Sequential()\r\n",
    "model_bigram.add(Dense(100, input_shape=(VOCAB_SIZE,)))\r\n",
    "model_bigram.add(Dense(100, activation='relu'))\r\n",
    "model_bigram.add(Dense(100, activation='relu'))\r\n",
    "model_bigram.add(Dense(VOCAB_SIZE, activation='softmax'))\r\n",
    "\r\n",
    "model_bigram.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\r\n",
    "model_bigram.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 100)               500200    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5001)              505101    \n",
      "=================================================================\n",
      "Total params: 1,025,501\n",
      "Trainable params: 1,025,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "for epoch in range(NUM_EPOCHS):\r\n",
    "    model_bigram.fit(X_train_bigram, y_train_bigram, batch_size=128, epochs=epoch+1, initial_epoch=epoch, validation_data=(X_dev_bigram, y_dev_bigram))\r\n",
    "    print(get_sentence(model_bigram, ['i', 'have'], context_size=2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 81366 samples, validate on 23837 samples\n",
      "Epoch 1/1\n",
      "32768/81366 [===========>..................] - ETA: 27s - loss: 6.0796 - acc: 0.1184"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ca4097c15951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel_bigram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_bigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_bigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev_bigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev_bigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_bigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'have'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/text-analytics/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/text-analytics/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    184\u001b[0m                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n\u001b[1;32m    185\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[0;32m~/anaconda3/envs/text-analytics/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/text-analytics/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "print(get_sentence(model_bigram, ['think', 'of'], context_size=2))\r\n",
    "print(get_sentence(model_bigram, ['well', 'we'], context_size=2))\r\n",
    "print(get_sentence(model_bigram, ['i', 'have'], context_size=2))\r\n",
    "print(get_sentence(model_bigram, ['who', 'will'], context_size=2))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "think of doting 5 mourn mourn officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer\n",
      "well we perish require officer perish officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer\n",
      "i have looks college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college\n",
      "who will shady silver clay officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trigram model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "X_train_trigram, y_train_trigram = build_bag_of_words(X_train, context_size=3)\r\n",
    "X_dev_trigram, y_dev_trigram = build_bag_of_words(X_dev, context_size=3)\r\n",
    "X_test_trigram, y_test_trigram = build_bag_of_words(X_test, context_size=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "print(X_train_trigram.shape)\r\n",
    "print(y_train_trigram.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(234077, 5001)\n",
      "(234077, 5001)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "model_trigram = Sequential()\r\n",
    "model_trigram.add(Dense(100, input_shape=(VOCAB_SIZE,)))\r\n",
    "model_trigram.add(Dense(100, activation='relu'))\r\n",
    "model_trigram.add(Dense(100, activation='relu'))\r\n",
    "model_trigram.add(Dense(VOCAB_SIZE, activation='softmax'))\r\n",
    "\r\n",
    "model_trigram.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\r\n",
    "model_trigram.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 100)               500200    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 5001)              505101    \n",
      "=================================================================\n",
      "Total params: 1,025,501\n",
      "Trainable params: 1,025,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "for epoch in range(NUM_EPOCHS):\r\n",
    "    model_trigram.fit(X_train_trigram, y_train_trigram, batch_size=128, epochs=epoch+1, initial_epoch=epoch, validation_data=(X_dev_trigram, y_dev_trigram))\r\n",
    "    print(get_sentence(model_trigram, ['i', 'have','to'], context_size=3))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 1/1\n",
      "234077/234077 [==============================] - 128s 546us/step - loss: 5.1627 - acc: 0.1738 - val_loss: 4.5636 - val_acc: 0.2223\n",
      "i have to be the __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 2/2\n",
      "234077/234077 [==============================] - 132s 566us/step - loss: 4.6285 - acc: 0.2311 - val_loss: 4.4308 - val_acc: 0.2419\n",
      "i have to be a __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 3/3\n",
      "234077/234077 [==============================] - 134s 571us/step - loss: 4.4008 - acc: 0.2486 - val_loss: 4.4002 - val_acc: 0.2478\n",
      "i have to __unk__ the __unk__ of __unk__ , and the __unk__ __unk__ of , and course i & apos ; s a __unk__ , __unk__ and the __unk__ __unk__ of , and course i & apos ; s a __unk__ , __unk__ and the __unk__ __unk__ of , and course i & apos ; s a __unk__ , __unk__ and the __unk__ __unk__ of , and course i & apos ; s a __unk__ , __unk__ and the __unk__ __unk__ of , and course i & apos ; s a __unk__ , __unk__ and the __unk__ __unk__ of ,\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 4/4\n",
      "234077/234077 [==============================] - 134s 570us/step - loss: 4.2256 - acc: 0.2597 - val_loss: 4.4195 - val_acc: 0.2503\n",
      "i have to be a __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 5/5\n",
      "234077/234077 [==============================] - 133s 567us/step - loss: 4.0682 - acc: 0.2675 - val_loss: 4.4739 - val_acc: 0.2511\n",
      "i have to __unk__ __unk__ the , __unk__ __unk__ and __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 6/6\n",
      "234077/234077 [==============================] - 137s 584us/step - loss: 3.9213 - acc: 0.2756 - val_loss: 4.5877 - val_acc: 0.2504\n",
      "i have to __unk__ __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 7/7\n",
      "234077/234077 [==============================] - 136s 580us/step - loss: 3.7823 - acc: 0.2840 - val_loss: 4.7392 - val_acc: 0.2505\n",
      "i have to __unk__ __unk__ the of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 8/8\n",
      "234077/234077 [==============================] - 136s 583us/step - loss: 3.6531 - acc: 0.2915 - val_loss: 4.9070 - val_acc: 0.2496\n",
      "i have to __unk__ __unk__ the of __unk__ __unk__ __unk__ , and i was __unk__ , __unk__ and i that & apos ; s a __unk__ of __unk__ __unk__ __unk__ , and i was __unk__ , __unk__ and i that & apos ; s a __unk__ of __unk__ __unk__ __unk__ , and i was __unk__ , __unk__ and i that & apos ; s a __unk__ of __unk__ __unk__ __unk__ , and i was __unk__ , __unk__ and i that & apos ; s a __unk__ of __unk__ __unk__ __unk__ , and i was __unk__ , __unk__ and i that\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 9/9\n",
      "234077/234077 [==============================] - 136s 582us/step - loss: 3.5330 - acc: 0.2998 - val_loss: 5.1068 - val_acc: 0.2457\n",
      "i have to __unk__ __unk__ the of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 10/10\n",
      "234077/234077 [==============================] - 137s 586us/step - loss: 3.4259 - acc: 0.3092 - val_loss: 5.3306 - val_acc: 0.2452\n",
      "i have to __unk__ __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 11/11\n",
      "234077/234077 [==============================] - 136s 583us/step - loss: 3.3299 - acc: 0.3185 - val_loss: 5.5235 - val_acc: 0.2446\n",
      "i have to do . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 12/12\n",
      "234077/234077 [==============================] - 136s 580us/step - loss: 3.2444 - acc: 0.3289 - val_loss: 5.6936 - val_acc: 0.2404\n",
      "i have to do . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 13/13\n",
      "234077/234077 [==============================] - 136s 580us/step - loss: 3.1696 - acc: 0.3392 - val_loss: 5.9064 - val_acc: 0.2369\n",
      "i have to do . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 14/14\n",
      "234077/234077 [==============================] - 137s 584us/step - loss: 3.1025 - acc: 0.3469 - val_loss: 6.0596 - val_acc: 0.2363\n",
      "i have to __unk__ __unk__ the of the united states , belshazzar , , , but the __unk__ of the the united states , belshazzar , , , but the __unk__ of the the united states , belshazzar , , , but the __unk__ of the the united states , belshazzar , , , but the __unk__ of the the united states , belshazzar , , , but the __unk__ of the the united states , belshazzar , , , but the __unk__ of the the united states , belshazzar , , , but the __unk__ of the the united\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 15/15\n",
      "234077/234077 [==============================] - 137s 584us/step - loss: 3.0416 - acc: 0.3551 - val_loss: 6.1924 - val_acc: 0.2327\n",
      "i have to do . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 16/16\n",
      "234077/234077 [==============================] - 136s 581us/step - loss: 2.9871 - acc: 0.3644 - val_loss: 6.3445 - val_acc: 0.2356\n",
      "i have to admit that , of we & apos ; s a __unk__ premise that is a __unk__ __unk__ of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 17/17\n",
      "234077/234077 [==============================] - 136s 582us/step - loss: 2.9383 - acc: 0.3714 - val_loss: 6.5133 - val_acc: 0.2321\n",
      "i have to __unk__ address the , u.s. and __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 18/18\n",
      "234077/234077 [==============================] - 135s 576us/step - loss: 2.8935 - acc: 0.3783 - val_loss: 6.7054 - val_acc: 0.2304\n",
      "i have to admit that , of course , , it & apos ; s a __unk__ in __unk__ the of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 19/19\n",
      "234077/234077 [==============================] - 137s 585us/step - loss: 2.8522 - acc: 0.3841 - val_loss: 6.8180 - val_acc: 0.2314\n",
      "i have to say a noble goodbye , a lot of people are deeply __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 20/20\n",
      "234077/234077 [==============================] - 136s 581us/step - loss: 2.8156 - acc: 0.3906 - val_loss: 6.9559 - val_acc: 0.2291\n",
      "i have to do with the iranians . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 21/21\n",
      "234077/234077 [==============================] - 136s 580us/step - loss: 2.7795 - acc: 0.3961 - val_loss: 7.0966 - val_acc: 0.2279\n",
      "i have to __unk__ a __unk__ , and i & apos ; s a __unk__ premise that says , & quot ; __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 22/22\n",
      "234077/234077 [==============================] - 133s 570us/step - loss: 2.7486 - acc: 0.4023 - val_loss: 7.2838 - val_acc: 0.2268\n",
      "i have to do . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 23/23\n",
      "234077/234077 [==============================] - 137s 584us/step - loss: 2.7184 - acc: 0.4065 - val_loss: 7.3507 - val_acc: 0.2256\n",
      "i have to __unk__ to the __unk__ __unk__ of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 24/24\n",
      "234077/234077 [==============================] - 137s 584us/step - loss: 2.6918 - acc: 0.4114 - val_loss: 7.4702 - val_acc: 0.2239\n",
      "i have to __unk__ a __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 25/25\n",
      "234077/234077 [==============================] - 131s 561us/step - loss: 2.6653 - acc: 0.4152 - val_loss: 7.5581 - val_acc: 0.2230\n",
      "i have to admit that we exist can __unk__ in __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 26/26\n",
      "234077/234077 [==============================] - 128s 546us/step - loss: 2.6418 - acc: 0.4196 - val_loss: 7.6382 - val_acc: 0.2256\n",
      "i have to say to that , be sounds __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 27/27\n",
      "234077/234077 [==============================] - 128s 545us/step - loss: 2.6199 - acc: 0.4229 - val_loss: 7.7873 - val_acc: 0.2248\n",
      "i have to admit that we exist __unk__ from the __unk__ __unk__ of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 28/28\n",
      "234077/234077 [==============================] - 134s 571us/step - loss: 2.5988 - acc: 0.4269 - val_loss: 7.9261 - val_acc: 0.2246\n",
      "i have to do with you something . & quot ; __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 29/29\n",
      "234077/234077 [==============================] - 137s 587us/step - loss: 2.5795 - acc: 0.4303 - val_loss: 8.0188 - val_acc: 0.2235\n",
      "i have to __unk__ to the __unk__ __unk__ of __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 30/30\n",
      "234077/234077 [==============================] - 137s 587us/step - loss: 2.5617 - acc: 0.4335 - val_loss: 8.1486 - val_acc: 0.2244\n",
      "i have to __unk__ to the __unk__ __unk__ of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 31/31\n",
      "234077/234077 [==============================] - 133s 568us/step - loss: 2.5433 - acc: 0.4371 - val_loss: 8.1420 - val_acc: 0.2193\n",
      "i have to admit that there are exceptions here does not be . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 32/32\n",
      "234077/234077 [==============================] - 135s 577us/step - loss: 2.5282 - acc: 0.4394 - val_loss: 8.3211 - val_acc: 0.2203\n",
      "i have to admit that . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 33/33\n",
      "234077/234077 [==============================] - 136s 582us/step - loss: 2.5119 - acc: 0.4416 - val_loss: 8.4854 - val_acc: 0.2220\n",
      "i have to admit that . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 34/34\n",
      "234077/234077 [==============================] - 137s 583us/step - loss: 2.4976 - acc: 0.4448 - val_loss: 8.5046 - val_acc: 0.2226\n",
      "i have to start answering . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 35/35\n",
      "234077/234077 [==============================] - 137s 584us/step - loss: 2.4850 - acc: 0.4469 - val_loss: 8.5823 - val_acc: 0.2192\n",
      "i have to say to , & quot ; __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 36/36\n",
      "234077/234077 [==============================] - 135s 579us/step - loss: 2.4713 - acc: 0.4494 - val_loss: 8.7226 - val_acc: 0.2209\n",
      "i have to admit that there are a few times . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 37/37\n",
      "234077/234077 [==============================] - 136s 583us/step - loss: 2.4595 - acc: 0.4521 - val_loss: 8.6879 - val_acc: 0.2204\n",
      "i have to admit that there are a quarter of million them , which is a ring road around london , recently , we & apos ; s a __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 38/38\n",
      "234077/234077 [==============================] - 135s 576us/step - loss: 2.4472 - acc: 0.4530 - val_loss: 8.8836 - val_acc: 0.2209\n",
      "i have to admit that . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 39/39\n",
      "234077/234077 [==============================] - 136s 582us/step - loss: 2.4366 - acc: 0.4547 - val_loss: 8.9757 - val_acc: 0.2194\n",
      "i have to say to , & quot ; __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 40/40\n",
      "234077/234077 [==============================] - 137s 586us/step - loss: 2.4263 - acc: 0.4564 - val_loss: 8.9936 - val_acc: 0.2182\n",
      "i have to say to myself , and i & apos ; s a __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 41/41\n",
      "234077/234077 [==============================] - 137s 585us/step - loss: 2.4154 - acc: 0.4587 - val_loss: 9.1049 - val_acc: 0.2195\n",
      "i have to do it , and & quot ; __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 42/42\n",
      "234077/234077 [==============================] - 136s 581us/step - loss: 2.4054 - acc: 0.4605 - val_loss: 9.1707 - val_acc: 0.2184\n",
      "i have to __unk__ to the __unk__ __unk__ of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 43/43\n",
      "234077/234077 [==============================] - 132s 566us/step - loss: 2.3955 - acc: 0.4616 - val_loss: 9.2490 - val_acc: 0.2194\n",
      "i have to __unk__ come out , and i & apos ; s a __unk__ of __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 44/44\n",
      "234077/234077 [==============================] - 136s 583us/step - loss: 2.3867 - acc: 0.4636 - val_loss: 9.3117 - val_acc: 0.2176\n",
      "i have to do it . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 45/45\n",
      "234077/234077 [==============================] - 134s 573us/step - loss: 2.3788 - acc: 0.4643 - val_loss: 9.3428 - val_acc: 0.2182\n",
      "i have to admit that we exist __unk__ from the textbook , it looked like . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 46/46\n",
      "234077/234077 [==============================] - 136s 583us/step - loss: 2.3707 - acc: 0.4664 - val_loss: 9.4899 - val_acc: 0.2161\n",
      "i have to admit that we are inevitably going to __unk__ the __unk__ of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 47/47\n",
      "234077/234077 [==============================] - 137s 587us/step - loss: 2.3622 - acc: 0.4672 - val_loss: 9.5189 - val_acc: 0.2170\n",
      "i have to admit that we exist __unk__ from , __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 48/48\n",
      "234077/234077 [==============================] - 137s 584us/step - loss: 2.3552 - acc: 0.4689 - val_loss: 9.6049 - val_acc: 0.2177\n",
      "i have to __unk__ tradition change , performance and raised truth is systems -- evidence which talent if you can find a tumor in the upper right quadrant , where performance is strong and learning opportunities are equally distributed . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 49/49\n",
      "234077/234077 [==============================] - 136s 582us/step - loss: 2.3469 - acc: 0.4709 - val_loss: 9.6810 - val_acc: 0.2164\n",
      "i have to admit that . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 50/50\n",
      "234077/234077 [==============================] - 136s 583us/step - loss: 2.3396 - acc: 0.4714 - val_loss: 9.7029 - val_acc: 0.2153\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "i have to admit that we exist __unk__ from , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "interpreter": {
   "hash": "fbbb7d2143a1d68e1cf272edf0974e702b621cb99b4ee39ce84db3bf0ffb588e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}