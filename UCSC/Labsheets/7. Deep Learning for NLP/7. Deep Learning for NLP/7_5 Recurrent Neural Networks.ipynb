{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from keras.datasets import reuters\r\n",
    "from keras.preprocessing.sequence import pad_sequences\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense, Softmax, Dropout\r\n",
    "from keras.layers import SimpleRNN, LSTM, Embedding, Bidirectional, GlobalAveragePooling1D\r\n",
    "from keras.utils import to_categorical\r\n",
    "\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Constants"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "MAX_SEQUENCE_LENGTH = 200 # We expect all sentences to be less than 200 tokens long\r\n",
    "VOCAB_SIZE = 10000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data loading\n",
    "In this exercise, we will use a smaller dataset that has been preprocessing already by the Keras folks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\",\r\n",
    "                                                         num_words=VOCAB_SIZE,\r\n",
    "                                                         skip_top=0,\r\n",
    "                                                         maxlen=MAX_SEQUENCE_LENGTH,\r\n",
    "                                                         test_split=0.5,\r\n",
    "                                                         seed=113,\r\n",
    "                                                         start_char=1,\r\n",
    "                                                         oov_char=2,\r\n",
    "                                                         index_from=3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 1s 0us/step\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Kavishka\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:148: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\Kavishka\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\reuters.py:149: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation for learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# News articles padded with zeros (in front here) to make 200 input vector (max sentence length)\r\n",
    "# The 200 corresponds to the number of time steps in the RNN\r\n",
    "# Default in Keras is to pad in front!\r\n",
    "X_train = pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH, value=0) \r\n",
    "X_test = pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH, value=0)\r\n",
    "\r\n",
    "print(X_train[:10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[   0    0    0 ...   15   17   12]\n",
      " [   0    0    0 ...  505   17   12]\n",
      " [   0    0    0 ...   11   17   12]\n",
      " ...\n",
      " [   0    0    0 ...  254   17   12]\n",
      " [   0    0    0 ... 2735   17   12]\n",
      " [   0    0    0 ... 4329   17   12]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple RNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "model = Sequential()\r\n",
    "# Special dense layer that does word embeddings - auto creates idx mapping\r\n",
    "# Length of our embeddings here is 10 - we feel 10 dimensions is sufficient to capture model\r\n",
    "model.add(Embedding(VOCAB_SIZE, 10, input_length=MAX_SEQUENCE_LENGTH)) \r\n",
    "# We specify that the RNN should have 25 hidden neurons; returns a vector of 25 at the end (summary)\r\n",
    "model.add(SimpleRNN(25)) \r\n",
    "model.add(Dense(46)) # Inputs 25 and outputs 46 (the number of classes we have)\r\n",
    "model.add(Softmax())\r\n",
    "\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\r\n",
    "model.summary()\r\n",
    "model.fit(X_train, to_categorical(y_train), epochs=10, validation_split=0.05)\r\n",
    "loss, acc = model.evaluate(X_test, to_categorical(y_test))\r\n",
    "print(\"Test accuracy: %0.2f%%\"%(acc*100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 10)           100000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 25)                900       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 46)                1196      \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 46)                0         \n",
      "=================================================================\n",
      "Total params: 102,096\n",
      "Trainable params: 102,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "132/132 [==============================] - 14s 84ms/step - loss: 3.1090 - acc: 0.2031 - val_loss: 1.9344 - val_acc: 0.4820\n",
      "Epoch 2/10\n",
      "132/132 [==============================] - 9s 68ms/step - loss: 2.2370 - acc: 0.4110 - val_loss: 1.9192 - val_acc: 0.4820\n",
      "Epoch 3/10\n",
      "132/132 [==============================] - 6s 47ms/step - loss: 2.2449 - acc: 0.4023 - val_loss: 1.9132 - val_acc: 0.4820\n",
      "Epoch 4/10\n",
      "132/132 [==============================] - 6s 48ms/step - loss: 2.1748 - acc: 0.4162 - val_loss: 1.8212 - val_acc: 0.5135\n",
      "Epoch 5/10\n",
      "132/132 [==============================] - 6s 44ms/step - loss: 1.9894 - acc: 0.4672 - val_loss: 1.7509 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "132/132 [==============================] - 6s 45ms/step - loss: 1.7527 - acc: 0.5366 - val_loss: 1.7596 - val_acc: 0.5225\n",
      "Epoch 7/10\n",
      "132/132 [==============================] - 6s 44ms/step - loss: 1.5612 - acc: 0.5938 - val_loss: 1.7737 - val_acc: 0.5180\n",
      "Epoch 8/10\n",
      "132/132 [==============================] - 6s 43ms/step - loss: 1.4203 - acc: 0.6332 - val_loss: 1.8607 - val_acc: 0.5360\n",
      "Epoch 9/10\n",
      "132/132 [==============================] - 5s 39ms/step - loss: 1.3510 - acc: 0.6436 - val_loss: 1.8795 - val_acc: 0.5405\n",
      "Epoch 10/10\n",
      "132/132 [==============================] - 6s 42ms/step - loss: 1.1863 - acc: 0.6944 - val_loss: 2.0224 - val_acc: 0.5270\n",
      "139/139 [==============================] - 2s 15ms/step - loss: 2.2730 - acc: 0.4872\n",
      "Test accuracy: 48.72%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bidirectional RNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "model = Sequential()\r\n",
    "model.add(Embedding(VOCAB_SIZE, 10, input_length=MAX_SEQUENCE_LENGTH))\r\n",
    "model.add(Bidirectional(SimpleRNN(25), merge_mode='ave'))\r\n",
    "model.add(Dense(46))\r\n",
    "model.add(Softmax())\r\n",
    "\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\r\n",
    "model.summary()\r\n",
    "model.fit(X_train, to_categorical(y_train), epochs=5, validation_split=0.05)\r\n",
    "loss, acc = model.evaluate(X_test, to_categorical(y_test))\r\n",
    "print(\"Test accuracy: %0.2f%%\"%(acc*100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 200, 10)           100000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 25)                1800      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 46)                1196      \n",
      "_________________________________________________________________\n",
      "softmax_2 (Softmax)          (None, 46)                0         \n",
      "=================================================================\n",
      "Total params: 102,996\n",
      "Trainable params: 102,996\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "132/132 [==============================] - 21s 115ms/step - loss: 3.1102 - acc: 0.3223 - val_loss: 1.9409 - val_acc: 0.4820\n",
      "Epoch 2/5\n",
      "132/132 [==============================] - 12s 88ms/step - loss: 2.2601 - acc: 0.4036 - val_loss: 1.9157 - val_acc: 0.4820\n",
      "Epoch 3/5\n",
      "132/132 [==============================] - 12s 88ms/step - loss: 2.1929 - acc: 0.4097 - val_loss: 1.9251 - val_acc: 0.4820\n",
      "Epoch 4/5\n",
      "132/132 [==============================] - 15s 109ms/step - loss: 2.2502 - acc: 0.3945 - val_loss: 1.9084 - val_acc: 0.4820\n",
      "Epoch 5/5\n",
      "132/132 [==============================] - 14s 109ms/step - loss: 2.2274 - acc: 0.4127 - val_loss: 1.9244 - val_acc: 0.4820\n",
      "139/139 [==============================] - 3s 19ms/step - loss: 2.2139 - acc: 0.4140\n",
      "Test accuracy: 41.40%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple RNN with averaging"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model = Sequential()\r\n",
    "model.add(Embedding(VOCAB_SIZE, 10, input_length=MAX_SEQUENCE_LENGTH))\r\n",
    "# Instead of returning the summary vector - we ask RNN to return vectors at each RNN unit\r\n",
    "model.add(SimpleRNN(25, return_sequences=True))\r\n",
    "# We ask the 25 output vectors to be averaged\r\n",
    "model.add(GlobalAveragePooling1D())\r\n",
    "model.add(Dense(46))\r\n",
    "model.add(Softmax())\r\n",
    "\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\r\n",
    "model.summary()\r\n",
    "model.fit(X_train, to_categorical(y_train), epochs=5, validation_split=0.05)\r\n",
    "loss, acc = model.evaluate(X_test, to_categorical(y_test))\r\n",
    "print(\"Test accuracy: %0.2f%%\"%(acc*100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 200, 10)           100000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 200, 25)           900       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 46)                1196      \n",
      "_________________________________________________________________\n",
      "softmax_3 (Softmax)          (None, 46)                0         \n",
      "=================================================================\n",
      "Total params: 102,096\n",
      "Trainable params: 102,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "132/132 [==============================] - 16s 85ms/step - loss: 3.1145 - acc: 0.3691 - val_loss: 1.9417 - val_acc: 0.4820\n",
      "Epoch 2/5\n",
      "132/132 [==============================] - 6s 46ms/step - loss: 2.2403 - acc: 0.4034 - val_loss: 1.8953 - val_acc: 0.4820\n",
      "Epoch 3/5\n",
      "132/132 [==============================] - 6s 46ms/step - loss: 2.1999 - acc: 0.4066 - val_loss: 1.7570 - val_acc: 0.5000\n",
      "Epoch 4/5\n",
      "132/132 [==============================] - 6s 46ms/step - loss: 2.1204 - acc: 0.4380 - val_loss: 1.6615 - val_acc: 0.5135\n",
      "Epoch 5/5\n",
      "132/132 [==============================] - 9s 71ms/step - loss: 1.9451 - acc: 0.4823 - val_loss: 1.5894 - val_acc: 0.5901\n",
      "139/139 [==============================] - 3s 22ms/step - loss: 1.8558 - acc: 0.5168\n",
      "Test accuracy: 51.68%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multilayer Bidirectional RNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model = Sequential()\r\n",
    "model.add(Embedding(VOCAB_SIZE, 10, input_length=MAX_SEQUENCE_LENGTH))\r\n",
    "# We ask keras to 'merge' (average) the vectors of both directions to send it to the next layer\r\n",
    "model.add(Bidirectional(LSTM(25, return_sequences=True), merge_mode='ave'))\r\n",
    "# The final bidirectional layer only needs the summary vector\r\n",
    "model.add(Bidirectional(LSTM(25), merge_mode='ave'))\r\n",
    "model.add(Dense(46))\r\n",
    "model.add(Softmax())\r\n",
    "\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\r\n",
    "model.summary()\r\n",
    "model.fit(X_train, to_categorical(y_train), epochs=5, validation_split=0.05)\r\n",
    "loss, acc = model.evaluate(X_test, to_categorical(y_test))\r\n",
    "print(\"Test accuracy: %0.2f%%\"%(acc*100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 200, 10)           100000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 25)           7200      \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 25)                10200     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 46)                1196      \n",
      "_________________________________________________________________\n",
      "softmax_4 (Softmax)          (None, 46)                0         \n",
      "=================================================================\n",
      "Total params: 118,596\n",
      "Trainable params: 118,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "132/132 [==============================] - 39s 203ms/step - loss: 3.1271 - acc: 0.3971 - val_loss: 1.9491 - val_acc: 0.4820\n",
      "Epoch 2/5\n",
      "132/132 [==============================] - 33s 252ms/step - loss: 2.2532 - acc: 0.4134 - val_loss: 1.9218 - val_acc: 0.4820\n",
      "Epoch 3/5\n",
      "132/132 [==============================] - 25s 192ms/step - loss: 2.2536 - acc: 0.4032 - val_loss: 1.9114 - val_acc: 0.4820\n",
      "Epoch 4/5\n",
      "132/132 [==============================] - 22s 168ms/step - loss: 2.2455 - acc: 0.4077 - val_loss: 1.9112 - val_acc: 0.4820\n",
      "Epoch 5/5\n",
      "132/132 [==============================] - 23s 172ms/step - loss: 2.2263 - acc: 0.4102 - val_loss: 1.9258 - val_acc: 0.4820\n",
      "139/139 [==============================] - 6s 44ms/step - loss: 2.2125 - acc: 0.4140\n",
      "Test accuracy: 41.40%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "interpreter": {
   "hash": "fbbb7d2143a1d68e1cf272edf0974e702b621cb99b4ee39ce84db3bf0ffb588e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}