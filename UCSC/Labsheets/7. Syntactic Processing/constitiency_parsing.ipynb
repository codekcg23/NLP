{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "fbbb7d2143a1d68e1cf272edf0974e702b621cb99b4ee39ce84db3bf0ffb588e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# LabSheet 6 \n",
    "## 17000475\n",
    "\n",
    "## Constituency Parsing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'the quick brown fox jumped over the lazy dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Kavishka\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: The StanfordParser will be deprecated\nPlease use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "java_path = 'C:/Program Files/Java/jdk-13.0.1'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "os.environ['CLASSPATH'] = 'F:/Downloads/stanford-parser-4.2.0/stanford-parser-full-2020-11-17'\n",
    "\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "scp = StanfordParser(model_path='edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(ROOT\n  (S\n    (NP (DT the) (JJ quick) (JJ brown) (NN fox))\n    (VP (VBD jumped) (PP (IN over) (NP (DT the) (JJ lazy) (NN dog))))))\n"
     ]
    }
   ],
   "source": [
    "result = list(scp.raw_parse(sentence))\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0].draw()"
   ]
  },
  {
   "source": [
    "![image](/Screenshot 2021-07-01 185522.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.grammar import Nonterminal\n",
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n  (NP-SBJ (NNP Mr.) (NNP Vinken))\n  (VP\n    (VBZ is)\n    (NP-PRD\n      (NP (NN chairman))\n      (PP\n        (IN of)\n        (NP\n          (NP (NNP Elsevier) (NNP N.V.))\n          (, ,)\n          (NP (DT the) (NNP Dutch) (VBG publishing) (NN group))))))\n  (. .))\n"
     ]
    }
   ],
   "source": [
    "training_set = treebank.parsed_sents()\n",
    "\n",
    "print(training_set[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[NN -> 'are',\n",
       " NNP -> 'Si',\n",
       " NNS -> 'bombs',\n",
       " CC -> 'or',\n",
       " VBZ -> 'rates',\n",
       " NN -> 'atmosphere',\n",
       " NNS -> 'write-downs',\n",
       " NNS -> 'Ringers',\n",
       " VBG -> 'soliciting',\n",
       " VP -> VBD NP PP-BNF]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# extract the productions for all annotated training sentences\n",
    "treebank_productions = list(\n",
    "                        set(production \n",
    "                            for sent in training_set  \n",
    "                            for production in sent.productions()\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "treebank_productions[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add productions for each word, POS tag\n",
    "for word, tag in treebank.tagged_words():\n",
    "\tt = nltk.Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\")\n",
    "\tfor production in t.productions():\n",
    "\t\ttreebank_productions.append(production)\n",
    "\n",
    "# build the PCFG based grammar  \n",
    "treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), \n",
    "                                         treebank_productions)\n",
    "\n",
    "# build the parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n",
    "\n",
    "# get sample sentence tokens\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# get parse tree for sample sentence\n",
    "#result = list(viterbi_parser.parse(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('the', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# get tokens and their POS tags from pattern package\n",
    "#from pattern.en import tag as pos_tagger\n",
    "#tagged_sent = pos_tagger(sentence)\n",
    "\n",
    "# use NLTK POS tagger instead\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens)\n",
    "\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n  (NP-SBJ-126 (DT the) (JJ quick) (NN brown) (NN fox))\n  (VP\n    (VBD jumped)\n    (ADVP-DIR (RP over))\n    (NP-TMP (DT the) (JJ lazy) (NN dog)))) (p=2.61338e-35)\n"
     ]
    }
   ],
   "source": [
    "# extend productions for sample sentence tokens\n",
    "for word, tag in tagged_sent:\n",
    "    t = nltk.Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\")\n",
    "    for production in t.productions():\n",
    "        treebank_productions.append(production)\n",
    "\n",
    "# rebuild grammar\n",
    "treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal('S'), \n",
    "                                         treebank_productions)                                         \n",
    "\n",
    "# rebuild parser\n",
    "viterbi_parser = nltk.ViterbiParser(treebank_grammar)\n",
    "\n",
    "# get parse tree for sample sentence\n",
    "result = list(viterbi_parser.parse(tokens))\n",
    "\n",
    "print(result[0])\n",
    "result[0].draw()"
   ]
  },
  {
   "source": [
    "![image2](Screenshot 2021-07-01 221702.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Dependency Parsing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]<---Colorless[]--->[]\n--------\n[]<---green[]--->[]\n--------\n[]<---ideas[]--->[]\n--------\n[]<---sleep[]--->[]\n--------\n[]<---furiously[]--->[]\n--------\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Colorless green ideas sleep furiously'\n",
    "\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "parsed_sent = parser(sentence)\n",
    "\n",
    "dependency_pattern = '{left}<---{word}[{w_type}]--->{right}\\n--------'\n",
    "for token in parsed_sent:\n",
    "    print(dependency_pattern.format(word=token.orth_, \n",
    "                                  w_type=token.dep_,\n",
    "                                  left=[t.orth_ \n",
    "                                            for t \n",
    "                                            in token.lefts],\n",
    "                                  right=[t.orth_ \n",
    "                                             for t \n",
    "                                             in token.rights]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Kavishka\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: The StanfordDependencyParser will be deprecated\nPlease use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "java_path = 'C:/Program Files/Java/jdk-13.0.1'\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "os.environ['CLASSPATH'] = 'F:/Downloads/stanford-parser-4.2.0/stanford-parser-full-2020-11-17'\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "sdp = StanfordDependencyParser(model_path='edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x000002047D312CA8>,\n            {0: {'address': 0,\n                 'ctag': 'TOP',\n                 'deps': defaultdict(<class 'list'>, {'root': [4]}),\n                 'feats': None,\n                 'head': None,\n                 'lemma': None,\n                 'rel': None,\n                 'tag': 'TOP',\n                 'word': None},\n             1: {'address': 1,\n                 'ctag': 'JJ',\n                 'deps': defaultdict(<class 'list'>, {}),\n                 'feats': '_',\n                 'head': 3,\n                 'lemma': '_',\n                 'rel': 'amod',\n                 'tag': 'JJ',\n                 'word': 'Colorless'},\n             2: {'address': 2,\n                 'ctag': 'JJ',\n                 'deps': defaultdict(<class 'list'>, {}),\n                 'feats': '_',\n                 'head': 3,\n                 'lemma': '_',\n                 'rel': 'amod',\n                 'tag': 'JJ',\n                 'word': 'green'},\n             3: {'address': 3,\n                 'ctag': 'NNS',\n                 'deps': defaultdict(<class 'list'>, {'amod': [1, 2]}),\n                 'feats': '_',\n                 'head': 4,\n                 'lemma': '_',\n                 'rel': 'nsubj',\n                 'tag': 'NNS',\n                 'word': 'ideas'},\n             4: {'address': 4,\n                 'ctag': 'VBP',\n                 'deps': defaultdict(<class 'list'>,\n                                     {'advmod': [5],\n                                      'nsubj': [3]}),\n                 'feats': '_',\n                 'head': 0,\n                 'lemma': '_',\n                 'rel': 'root',\n                 'tag': 'VBP',\n                 'word': 'sleep'},\n             5: {'address': 5,\n                 'ctag': 'RB',\n                 'deps': defaultdict(<class 'list'>, {}),\n                 'feats': '_',\n                 'head': 4,\n                 'lemma': '_',\n                 'rel': 'advmod',\n                 'tag': 'RB',\n                 'word': 'furiously'}})\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "Cannot find the dot binary from Graphviz package",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\dependencygraph.py\u001b[0m in \u001b[0;36m_repr_svg_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    212\u001b[0m                 \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                 \u001b[0muniversal_newlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[0;32m    799\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    801\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1206\u001b[0m                                          \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcwd\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcwd\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1207\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1208\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\dependencygraph.py\u001b[0m in \u001b[0;36m_repr_svg_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    214\u001b[0m             )\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot find the dot binary from Graphviz package'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdot_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Cannot find the dot binary from Graphviz package"
     ]
    }
   ],
   "source": [
    "from graphviz import Source\n",
    "result = list(sdp.raw_parse(sentence))\n",
    "dep_tree = [parse for parse in result][0]\n",
    "print(dep_tree)\n",
    "dep_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_tree_dot_repr = [parse for parse in result][0].to_dot()\n",
    "source = Source(dep_tree_dot_repr, filename=\"dep_tree\", format=\"png\")\n",
    "#source.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dependency grammar with 12 productions\n  'fox' -> 'The'\n  'fox' -> 'brown'\n  'quick' -> 'fox'\n  'quick' -> 'is'\n  'quick' -> 'and'\n  'quick' -> 'jumping'\n  'jumping' -> 'he'\n  'jumping' -> 'is'\n  'jumping' -> 'dog'\n  'dog' -> 'over'\n  'dog' -> 'the'\n  'dog' -> 'lazy'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "dependency_rules = \"\"\"\n",
    "'fox' -> 'The' | 'brown'\n",
    "'quick' -> 'fox' | 'is' | 'and' | 'jumping'\n",
    "'jumping' -> 'he' | 'is' | 'dog'\n",
    "'dog' -> 'over' | 'the' | 'lazy'\n",
    "\"\"\"\n",
    "\n",
    "dependency_grammar = nltk.grammar.DependencyGrammar.fromstring(dependency_rules)\n",
    "print(dependency_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Colorless', 'green', 'ideas', 'sleep', 'furiously']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "dp = nltk.ProjectiveDependencyParser(dependency_grammar)\n",
    "res = [item for item in dp.parse(tokens)]\n",
    "res\n",
    "tree = res[1] \n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.draw()   "
   ]
  },
  {
   "source": [
    "## POS Tagging"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('I', 'PRON'), ('saw', 'VERB'), ('the', 'DET'), ('man', 'NOUN'), ('with', 'ADP'), ('the', 'DET'), ('telescope', 'NOUN'), ('but', 'CONJ'), ('he', 'PRON'), ('did', 'VERB'), (\"n't\", 'ADV'), ('see', 'VERB'), ('me', 'PRON')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I saw the man with the telescope but he didn't see me\"\n",
    "\n",
    "\n",
    "# Using NLTK's built-in tagger based on PTB\n",
    "import nltk\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens, tagset='universal')\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank\n",
    "data = treebank.tagged_sents()\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "414"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# SAQ 1: How much data is there for training, testing? \n",
    "# rain - 3500 and Test - 414\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('acquisition', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('87-store', 'JJ'),\n",
       " ('Weisfield', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('raises', 'VBZ'),\n",
       " ('Ratners', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('U.S.', 'NNP'),\n",
       " ('presence', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('450', 'CD'),\n",
       " ('stores', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# SAQ 2: What is the last training sentence; test sentence?\n",
    "train_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Trinity', 'NNP'),\n",
       " ('said', 'VBD'),\n",
       " ('0', '-NONE-'),\n",
       " ('it', 'PRP'),\n",
       " ('plans', 'VBZ'),\n",
       " ('*-1', '-NONE-'),\n",
       " ('to', 'TO'),\n",
       " ('begin', 'VB'),\n",
       " ('delivery', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('first', 'JJ'),\n",
       " ('quarter', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('next', 'JJ'),\n",
       " ('year', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# SAQ 2: What is the last training sentence; test sentence?\n",
    "test_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.1454158195372253\n[('I', 'NN'), ('saw', 'NN'), ('the', 'NN'), ('man', 'NN'), ('with', 'NN'), ('the', 'NN'), ('telescope', 'NN'), ('but', 'NN'), ('he', 'NN'), ('did', 'NN'), (\"n't\", 'NN'), ('see', 'NN'), ('me', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Default 'naive' tagger - tags all words with a given tag!\n",
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN') # Can specify any default tag - NN gives best score - why? yes. It has most of the data\n",
    "# Test score and example sentence tag output\n",
    "print(dt.evaluate(test_data))\n",
    "print(dt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.24039113176493368\n[('I', 'NN'), ('saw', 'NN'), ('the', 'NN'), ('man', 'NN'), ('with', 'NN'), ('the', 'NN'), ('telescope', 'NN'), ('but', 'NN'), ('he', 'NN'), ('did', 'NN'), (\"n't\", 'NN'), ('see', 'NN'), ('me', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Regex tagger\n",
    "from nltk.tag import RegexpTagger\n",
    "# Define 'fixed' regex tag patterns\n",
    "patterns = [\n",
    "        (r'.*ing$', 'VBG'),               # gerunds\n",
    "        (r'.*ed$', 'VBD'),                # simple past\n",
    "        (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "        (r'.*ould$', 'MD'),               # modals\n",
    "        (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "        (r'.*s$', 'NNS'),                 # plural nouns\n",
    "        (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "        (r'.*', 'NN')                     # nouns (default) ... \n",
    "]\n",
    "rt = RegexpTagger(patterns)\n",
    "\n",
    "# Test score and example sentence tag output\n",
    "print(rt.evaluate(test_data))\n",
    "print(rt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training your own tagger\n",
    "# 1. using n-gram taggers and combining them with backoff\n",
    "# 2. using naive bayes (statistical) model\n",
    "# 3. using maximum entropy (classifier) model\n",
    "\n",
    "## N gram taggers\n",
    "from nltk.tag import UnigramTagger # Context insentitive\n",
    "from nltk.tag import BigramTagger  # Considers previous word\n",
    "from nltk.tag import TrigramTagger # Considers previous 2 words\n",
    "\n",
    "# Traing the taggers\n",
    "ut = UnigramTagger(train_data)\n",
    "bt = BigramTagger(train_data)\n",
    "tt = TrigramTagger(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8607803272340013\n",
      "[('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('man', 'NN'), ('with', 'IN'), ('the', 'DT'), ('telescope', None), ('but', 'CC'), ('he', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('see', 'VB'), ('me', 'PRP')]\n",
      "0.13466937748087907\n",
      "[('I', 'PRP'), ('saw', None), ('the', None), ('man', None), ('with', None), ('the', None), ('telescope', None), ('but', None), ('he', None), ('did', None), (\"n't\", None), ('see', None), ('me', None)]\n",
      "0.08064672281924679\n",
      "[('I', 'PRP'), ('saw', None), ('the', None), ('man', None), ('with', None), ('the', None), ('telescope', None), ('but', None), ('he', None), ('did', None), (\"n't\", None), ('see', None), ('me', None)]\n"
     ]
    }
   ],
   "source": [
    "# Test UnigramTagger score and example sentence tag output\n",
    "print(ut.evaluate(test_data))\n",
    "print(ut.tag(tokens))\n",
    "\n",
    "# Test BigramTagger score and example sentence tag output\n",
    "print(bt.evaluate(test_data))\n",
    "print(bt.tag(tokens))\n",
    "\n",
    "# Test TrigramTagger score and example sentence tag output\n",
    "print(tt.evaluate(test_data))\n",
    "print(tt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9094781682641108\n[('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('man', 'NN'), ('with', 'IN'), ('the', 'DT'), ('telescope', 'NN'), ('but', 'CC'), ('he', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('see', 'VB'), ('me', 'PRP')]\n"
     ]
    }
   ],
   "source": [
    "# Combining all 3 n-gram taggers with backoff (smoothing)\n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "ct = combined_tagger(train_data=train_data, \n",
    "                     taggers=[UnigramTagger, BigramTagger, TrigramTagger],\n",
    "                     backoff=rt)\n",
    "\n",
    "# Test Combined n-gram tagger score and example sentence tag output\n",
    "print(ct.evaluate(test_data))       \n",
    "print(ct.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9306806079969019\n[('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('man', 'NN'), ('with', 'IN'), ('the', 'DT'), ('telescope', 'NN'), ('but', 'CC'), ('he', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('see', 'VB'), ('me', 'PRP')]\n"
     ]
    }
   ],
   "source": [
    "# Treating POS tagging as a classification problem\n",
    "# We use the ClassifierBasedPOSTagger class to build a classifier by specifying some\n",
    "# classification algorithm - here the NaiveBayes abd Maxent algorithms which are passed\n",
    "# to the class via the classifier_builder parameter\n",
    "from nltk.classify import NaiveBayesClassifier, MaxentClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "# First a Naive Bayes (statistical) classifier\n",
    "nbt = ClassifierBasedPOSTagger(train=train_data,\n",
    "                               classifier_builder=NaiveBayesClassifier.train)\n",
    "\n",
    "# Test NBC tagger score and example sentence tag output\n",
    "print(nbt.evaluate(test_data))\n",
    "print(nbt.tag(tokens))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  ==> Training (10 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -3.82864        0.007\n",
      "             2          -0.76176        0.957\n",
      "C:\\Users\\Kavishka\\anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py:1392: RuntimeWarning: overflow encountered in power\n",
      "  exp_nf_delta = 2 ** nf_delta\n",
      "C:\\Users\\Kavishka\\anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py:1394: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum1 = numpy.sum(exp_nf_delta * A, axis=0)\n",
      "C:\\Users\\Kavishka\\anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py:1395: RuntimeWarning: invalid value encountered in multiply\n",
      "  sum2 = numpy.sum(nf_exp_nf_delta * A, axis=0)\n",
      "         Final               nan        0.984\n",
      "0.9270984606447865\n",
      "[('I', 'PRP'), ('saw', 'VBD'), ('the', 'DT'), ('man', 'NN'), ('with', 'IN'), ('the', 'DT'), ('telescope', 'NN'), ('but', 'CC'), ('he', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('see', 'VB'), ('me', 'PRP')]\n"
     ]
    }
   ],
   "source": [
    "# Finally a Maximum entropy classifier (that would take sometime)\n",
    "# met = ClassifierBasedPOSTagger(train=train_data,\n",
    "#                               classifier_builder=MaxentClassifier.train)\n",
    "\n",
    "met = ClassifierBasedPOSTagger(train=train_data, \n",
    "                               classifier_builder=lambda train_feats: MaxentClassifier.train(train_feats, max_iter=10))\n",
    "\n",
    "# Test Maxent tagger score and example sentence tag output\n",
    "print(met.evaluate(test_data))                           \n",
    "print(met.tag(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tagger accuracies:\n",
      "\n",
      "Default tagger 0.15\n",
      "Regex tagger 0.24\n",
      "Unigram tagger 0.86\n",
      "Bigram tagger 0.13\n",
      "Trigram tagger 0.08\n",
      "Combined tagger 0.91\n",
      "Naive Bayes tagger 0.93\n",
      "Maxent tagger 0.93\n"
     ]
    }
   ],
   "source": [
    "# Final accuracies\n",
    "print('Tagger accuracies:')\n",
    "print()\n",
    "print('Default tagger %.2f' %dt.evaluate(test_data))\n",
    "print('Regex tagger %.2f' %rt.evaluate(test_data))\n",
    "print('Unigram tagger %.2f' %ut.evaluate(test_data))\n",
    "print('Bigram tagger %.2f' %bt.evaluate(test_data))\n",
    "print('Trigram tagger %.2f' %tt.evaluate(test_data))\n",
    "print('Combined tagger %.2f' %ct.evaluate(test_data))\n",
    "print('Naive Bayes tagger %.2f' %nbt.evaluate(test_data))\n",
    "print('Maxent tagger %.2f' %met.evaluate(test_data))\n"
   ]
  },
  {
   "source": [
    "## Shallow_parsing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n  (NP Pierre/NNP Vinken/NNP)\n  ,/,\n  (NP 61/CD years/NNS)\n  old/JJ\n  ,/,\n  will/MD\n  join/VB\n  (NP the/DT board/NN)\n  as/IN\n  (NP a/DT nonexecutive/JJ director/NN Nov./NNP 29/CD)\n  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Training your own chunker using chunked treeband data - again made available in NLTK!\n",
    "# As before (with tagging) we first divide the data into training and testing sets\n",
    "from nltk.corpus import treebank_chunk\n",
    "data = treebank_chunk.chunked_sents()\n",
    "train_data = data[0:3500]\n",
    "test_data = data[3500:]\n",
    "print(train_data[0])\n",
    "\n",
    "simple_sentence = 'the brown fox jumped over the lazy dog'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('the', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('jumped', 'VBD'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Can use tagger from package pattern.en if using Python 2.x\n",
    "# from pattern.en import tag\n",
    "# tagged_sentence = tag(sentence)\n",
    "\n",
    "import nltk\n",
    "from nltk.chunk import RegexpParser\n",
    "tokens = nltk.word_tokenize(simple_sentence)\n",
    "tagged_simple_sent = nltk.pos_tag(tokens)\n",
    "print(tagged_simple_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n  (NP the/DT brown/JJ fox/NN)\n  (VP jumped/VBD over/IN)\n  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "# We first define our grammars using regex pattern using the RegexpParser\n",
    "# We can specify which patterns we want to segment in a sentence as *chunks*\n",
    "chunk_grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>*<NN.*>}\n",
    "VP: {<VBD><IN>}\n",
    "\"\"\"\n",
    "rc = RegexpParser(chunk_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n  the/DT\n  (NP brown/JJ fox/NN)\n  jumped/VBD\n  over/IN\n  the/DT\n  (NP lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "# We sometimes want to specify which patterns we DO NOT want to segment in a sentence\n",
    "# so that we can *chunk* all the others\n",
    "chink_grammar = \"\"\"\n",
    "NP: {<JJ|NN>+} # chunk only adjective-noun pair as NP\n",
    "\"\"\"\n",
    "\n",
    "rc = RegexpParser(chink_grammar)\n",
    "c = rc.parse(tagged_simple_sent)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n",
      "  (NP the/DT brown/JJ fox/NN)\n",
      "  (VP is/VBZ)\n",
      "  (ADJP quick/JJ)\n",
      "  and/CC\n",
      "  he/PRP\n",
      "  (VP may/MD jump/VB)\n",
      "  (PP over/IN)\n",
      "  (NP the/DT lazy/JJ dog/NN))\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  46.1%%\n",
      "    Precision:     19.9%%\n",
      "    Recall:        43.3%%\n",
      "    F-Measure:     27.3%%\n"
     ]
    }
   ],
   "source": [
    "# A more realistic grammar for chunking\n",
    "grammar = \"\"\"\n",
    "NP: {<DT>?<JJ>?<NN.*>}  \n",
    "ADJP: {<JJ>}\n",
    "ADVP: {<RB.*>}\n",
    "PP: {<IN>}      \n",
    "VP: {<MD>?<VB.*>+}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# And a more realistic sentence as input\n",
    "sentence = 'the brown fox is quick and he may jump over the lazy dog'\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens)\n",
    "\n",
    "rc = RegexpParser(grammar)\n",
    "c = rc.parse(tagged_sent)\n",
    "print(c)\n",
    "print(rc.evaluate(test_data))\n",
    "# The performance is not great!\n",
    "# Why is this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n  (NP A/DT Lorillard/NNP spokewoman/NN)\n  said/VBD\n  ,/,\n  ``/``\n  (NP This/DT)\n  is/VBZ\n  (NP an/DT old/JJ story/NN)\n  ./.)\n"
     ]
    }
   ],
   "source": [
    "# We have acecss to a utility function tree2conlltags which extracts word, tag and\n",
    "# chunk triples from annotated text\n",
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "# Let's take a slightly more typical sentence from our data\n",
    "train_sent = train_data[7]\n",
    "print(train_sent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('A', 'DT', 'B-NP'),\n",
       " ('Lorillard', 'NNP', 'I-NP'),\n",
       " ('spokewoman', 'NN', 'I-NP'),\n",
       " ('said', 'VBD', 'O'),\n",
       " (',', ',', 'O'),\n",
       " ('``', '``', 'O'),\n",
       " ('This', 'DT', 'B-NP'),\n",
       " ('is', 'VBZ', 'O'),\n",
       " ('an', 'DT', 'B-NP'),\n",
       " ('old', 'JJ', 'I-NP'),\n",
       " ('story', 'NN', 'I-NP'),\n",
       " ('.', '.', 'O')]"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# We extract the POS and chunk tags using tree2conlltags function which returns a list of tuples\n",
    "wtc = tree2conlltags(train_sent)\n",
    "wtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n  (NP A/DT Lorillard/NNP spokewoman/NN)\n  said/VBD\n  ,/,\n  ``/``\n  (NP This/DT)\n  is/VBZ\n  (NP an/DT old/JJ story/NN)\n  ./.)\n"
     ]
    }
   ],
   "source": [
    "# We can 'reverse' this to output a shallow tree using the conlltags2tree function\n",
    "tree = conlltags2tree(wtc)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use these features to train a 'combined' chunker as we did for POS tagging\n",
    "def conll_tag_chunks(chunk_sents):\n",
    "  tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "  return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
    "  \n",
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    "\n",
    "# We create a new class to use the word, POS and Chunk tag features to train a chunker\n",
    "# that is able to 'backoff' from bigram to a unigram model as before\n",
    "# Can you have another layer for trigram and back off to this model?\n",
    "class NGramTagChunker(ChunkParserI):\n",
    "    \n",
    "  def __init__(self, train_sentences, \n",
    "               tagger_classes=[UnigramTagger, BigramTagger]):\n",
    "    train_sent_tags = conll_tag_chunks(train_sentences)\n",
    "    self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes)\n",
    "\n",
    "  def parse(self, tagged_sentence):\n",
    "    if not tagged_sentence: \n",
    "        return None\n",
    "    pos_tags = [tag for word, tag in tagged_sentence]\n",
    "    chunk_pos_tags = self.chunk_tagger.tag(pos_tags)\n",
    "    chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags]\n",
    "    wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag)\n",
    "                     in zip(tagged_sentence, chunk_tags)]\n",
    "    return conlltags2tree(wpc_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ChunkParse score:\n    IOB Accuracy:  97.2%%\n    Precision:     91.4%%\n    Recall:        94.3%%\n    F-Measure:     92.8%%\n"
     ]
    }
   ],
   "source": [
    "# We call our new class and pass it the training data from the chunked treebank\n",
    "ntc = NGramTagChunker(train_data)\n",
    "print(ntc.evaluate(test_data))\n",
    "# Now we get really good results on the data set\n",
    "# Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n  (NP the/DT brown/JJ fox/NN)\n  is/VBZ\n  (NP quick/JJ)\n  and/CC\n  (NP he/PRP)\n  may/MD\n  jump/VB\n  over/IN\n  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "# Let's try to visualize the chunk for the more realistic sample sentence\n",
    "tree = ntc.parse(tagged_sent)\n",
    "print(tree)\n",
    "tree.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![image3]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(S\n  (NP He/PRP)\n  (VP reckons/VBZ)\n  (NP the/DT current/JJ account/NN deficit/NN)\n  (VP will/MD narrow/VB)\n  (PP to/TO)\n  (NP only/RB #/# 1.8/CD billion/CD)\n  (PP in/IN)\n  (NP September/NNP)\n  ./.)\n"
     ]
    }
   ],
   "source": [
    "# We now use our shallow parser on a larger 'Wall Street Journal' corpus\n",
    "from nltk.corpus import conll2000\n",
    "wsj_data = conll2000.chunked_sents()\n",
    "train_wsj_data = wsj_data[:7500]\n",
    "test_wsj_data = wsj_data[7500:]\n",
    "print(train_wsj_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10948"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "# SAQ 1. How big is it?\n",
    "len(wsj_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3448"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "# SAQ 2. How much test data did we have before, and how much now?\n",
    "len(test_wsj_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ChunkParse score:\n    IOB Accuracy:  89.4%%\n    Precision:     80.8%%\n    Recall:        86.0%%\n    F-Measure:     83.3%%\n"
     ]
    }
   ],
   "source": [
    "# We first train our model on the training corpus\n",
    "tc = NGramTagChunker(train_wsj_data)\n",
    "# And then we test it on the test data\n",
    "print(tc.evaluate(test_wsj_data))"
   ]
  }
 ]
}