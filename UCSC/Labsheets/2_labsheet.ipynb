{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd0fbbb7d2143a1d68e1cf272edf0974e702b621cb99b4ee39ce84db3bf0ffb588e",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total categories:  15\n"
     ]
    }
   ],
   "source": [
    "print(\"Total categories: \",len(brown.categories()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['There', 'were', 'thirty-eight', 'patients', 'on', 'the', 'bus', 'the', 'morning', 'I', 'left', 'for', 'Hanover', ',', 'most', 'of', 'them', 'disturbed', 'and', 'hallucinating', '.'], ['An', 'interne', ',', 'a', 'nurse', 'and', 'two', 'attendants', 'were', 'in', 'charge', 'of', 'us', '.'], ...]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Tokenize sentences in mystery category\n",
    "brown.sents(categories=\"mystery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[('There', 'EX'), ('were', 'BED'), ('thirty-eight', 'CD'), ('patients', 'NNS'), ('on', 'IN'), ('the', 'AT'), ('bus', 'NN'), ('the', 'AT'), ('morning', 'NN'), ('I', 'PPSS'), ('left', 'VBD'), ('for', 'IN'), ('Hanover', 'NP'), (',', ','), ('most', 'AP'), ('of', 'IN'), ('them', 'PPO'), ('disturbed', 'VBN'), ('and', 'CC'), ('hallucinating', 'VBG'), ('.', '.')], [('An', 'AT'), ('interne', 'NN'), (',', ','), ('a', 'AT'), ('nurse', 'NN'), ('and', 'CC'), ('two', 'CD'), ('attendants', 'NNS'), ('were', 'BED'), ('in', 'IN'), ('charge', 'NN'), ('of', 'IN'), ('us', 'PPO'), ('.', '.')], ...]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "brown.tagged_sents(categories='mystery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['There were thirty-eight patients on the bus the morning I left for Hanover , most of them disturbed and hallucinating .', 'An interne , a nurse and two attendants were in charge of us .', \"I felt lonely and depressed as I stared out the bus window at Chicago's grim , dirty West Side .\", 'It seemed incredible , as I listened to the monotonous drone of voices and smelled the fetid odors coming from the patients , that technically I was a ward of the state of Illinois , going to a hospital for the mentally ill .', 'I suddenly thought of Mary Jane Brennan , the way her pretty eyes could flash with anger , her quiet competence , the gentleness and sweetness that lay just beneath the surface of her defenses .']\n"
     ]
    }
   ],
   "source": [
    "sentences = brown.sents(categories='mystery')\n",
    "sentences = [' '.join(sentence_token) for sentence_token in sentences]\n",
    "print(sentences[0:5]) # printing first 5 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('patients', 'NNS'), ('bus', 'NN'), ('morning', 'NN'), ('Hanover', 'NP'), ('interne', 'NN'), ('nurse', 'NN'), ('attendants', 'NNS'), ('charge', 'NN'), ('bus', 'NN'), ('window', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tagged_words = brown.tagged_words(categories='mystery')\n",
    "nouns = [(word, tag) for word, tag in tagged_words if any(noun_tag in tag for noun_tag in ['NP', 'NN'])]\n",
    "print(nouns[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['acq',\n",
       " 'alum',\n",
       " 'barley',\n",
       " 'bop',\n",
       " 'carcass',\n",
       " 'castor-oil',\n",
       " 'cocoa',\n",
       " 'coconut',\n",
       " 'coconut-oil',\n",
       " 'coffee',\n",
       " 'copper',\n",
       " 'copra-cake',\n",
       " 'corn',\n",
       " 'cotton',\n",
       " 'cotton-oil',\n",
       " 'cpi',\n",
       " 'cpu',\n",
       " 'crude',\n",
       " 'dfl',\n",
       " 'dlr',\n",
       " 'dmk',\n",
       " 'earn',\n",
       " 'fuel',\n",
       " 'gas',\n",
       " 'gnp',\n",
       " 'gold',\n",
       " 'grain',\n",
       " 'groundnut',\n",
       " 'groundnut-oil',\n",
       " 'heat',\n",
       " 'hog',\n",
       " 'housing',\n",
       " 'income',\n",
       " 'instal-debt',\n",
       " 'interest',\n",
       " 'ipi',\n",
       " 'iron-steel',\n",
       " 'jet',\n",
       " 'jobs',\n",
       " 'l-cattle',\n",
       " 'lead',\n",
       " 'lei',\n",
       " 'lin-oil',\n",
       " 'livestock',\n",
       " 'lumber',\n",
       " 'meal-feed',\n",
       " 'money-fx',\n",
       " 'money-supply',\n",
       " 'naphtha',\n",
       " 'nat-gas',\n",
       " 'nickel',\n",
       " 'nkr',\n",
       " 'nzdlr',\n",
       " 'oat',\n",
       " 'oilseed',\n",
       " 'orange',\n",
       " 'palladium',\n",
       " 'palm-oil',\n",
       " 'palmkernel',\n",
       " 'pet-chem',\n",
       " 'platinum',\n",
       " 'potato',\n",
       " 'propane',\n",
       " 'rand',\n",
       " 'rape-oil',\n",
       " 'rapeseed',\n",
       " 'reserves',\n",
       " 'retail',\n",
       " 'rice',\n",
       " 'rubber',\n",
       " 'rye',\n",
       " 'ship',\n",
       " 'silver',\n",
       " 'sorghum',\n",
       " 'soy-meal',\n",
       " 'soy-oil',\n",
       " 'soybean',\n",
       " 'strategic-metal',\n",
       " 'sugar',\n",
       " 'sun-meal',\n",
       " 'sun-oil',\n",
       " 'sunseed',\n",
       " 'tea',\n",
       " 'tin',\n",
       " 'trade',\n",
       " 'veg-oil',\n",
       " 'wheat',\n",
       " 'wpi',\n",
       " 'yen',\n",
       " 'zinc']"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "reuters.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['test/16118', 'test/18534', 'test/18540', 'test/18664', 'test/18665', 'test/18672', 'test/18911', 'test/19875', 'test/20106', 'test/20116', 'training/1035', 'training/1036', 'training/10602', 'training/10604', 'training/11170', 'training/11665', 'training/2618', 'training/29', 'training/3105', 'training/3708', 'training/3720', 'training/3723', 'training/3898', 'training/5883', 'training/5886', 'training/6000', 'training/6067', 'training/6197', 'training/7005', 'training/7006', 'training/7015', 'training/7036', 'training/7098', 'training/7099', 'training/9615']\n"
     ]
    }
   ],
   "source": [
    "# fileid based access\n",
    "print(reuters.fileids(categories=['housing', 'income']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['YUGOSLAV', 'ECONOMY', 'WORSENED', 'IN', '1986', ',', 'BANK', 'DATA', 'SHOWS', 'National', 'Bank', 'economic', 'data', 'for', '1986', 'shows', 'that', 'Yugoslavia', \"'\", 's', 'trade', 'deficit', 'grew', ',', 'the', 'inflation', 'rate', 'rose', ',', 'wages', 'were', 'sharply', 'higher', ',', 'the', 'money', 'supply', 'expanded', 'and', 'the', 'value', 'of', 'the', 'dinar', 'fell', '.'], ['The', 'trade', 'deficit', 'for', '1986', 'was', '2', '.', '012', 'billion', 'dlrs', ',', '25', '.', '7', 'pct', 'higher', 'than', 'in', '1985', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "print(reuters.sents(fileids=[u'test/16118', u'test/18534']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Synset('hike.n.01'), Synset('rise.n.09'), Synset('raise.n.01'), Synset('hike.v.01'), Synset('hike.v.02')]\n"
     ]
    }
   ],
   "source": [
    "# load the Wordnet Corpus\n",
    "from nltk.corpus import wordnet as wn\n",
    "word = 'hike' # taking hike as our word of interest\n",
    "# get word synsets\n",
    "word_synsets = wn.synsets(word)\n",
    "print(word_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Synset Name: hike.n.01\nPOS Tag: n\nDefinition: a long walk usually for exercise or pleasure\nExamples: ['she enjoys a hike in her spare time']\n\nSynset Name: rise.n.09\nPOS Tag: n\nDefinition: an increase in cost\nExamples: ['they asked for a 10% rise in rates']\n\nSynset Name: raise.n.01\nPOS Tag: n\nDefinition: the amount a salary is increased\nExamples: ['he got a 3% raise', 'he got a wage hike']\n\nSynset Name: hike.v.01\nPOS Tag: v\nDefinition: increase\nExamples: ['The landlord hiked up the rents']\n\nSynset Name: hike.v.02\nPOS Tag: v\nDefinition: walk a long way, as for pleasure or physical exercise\nExamples: ['We were hiking in Colorado', 'hike the Rockies']\n\n"
     ]
    }
   ],
   "source": [
    "# get details for each synonym in synset\n",
    "for synset in word_synsets:\n",
    "    print('Synset Name:', synset.name())\n",
    "    print('POS Tag:', synset.pos())\n",
    "    print('Definition:', synset.definition())\n",
    "    print('Examples:', synset.examples())\n",
    "    print()"
   ]
  },
  {
   "source": [
    "## Frequency Distributions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "file1 = nltk.corpus.gutenberg.fileids( ) [0]\n",
    "emmatext = nltk.corpus.gutenberg.raw(file1)\n",
    "emmatokens = nltk.wordpunct_tokenize(emmatext)\n",
    "emmawords = [w.lower( ) for w in emmatokens]\n",
    "emmawords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(',', 11454),\n",
       " ('.', 6928),\n",
       " ('to', 5239),\n",
       " ('the', 5201),\n",
       " ('and', 4896),\n",
       " ('of', 4291),\n",
       " ('i', 3178),\n",
       " ('a', 3129),\n",
       " ('it', 2528),\n",
       " ('her', 2469),\n",
       " ('was', 2398),\n",
       " ('she', 2340),\n",
       " (';', 2199),\n",
       " ('in', 2188),\n",
       " ('not', 2140),\n",
       " ('\"', 2004),\n",
       " ('you', 1980),\n",
       " ('be', 1975),\n",
       " ('that', 1806),\n",
       " ('he', 1806),\n",
       " ('had', 1624),\n",
       " ('but', 1441),\n",
       " ('as', 1436),\n",
       " ('--', 1382),\n",
       " ('for', 1347),\n",
       " ('have', 1320),\n",
       " ('is', 1240),\n",
       " ('with', 1217),\n",
       " ('very', 1202),\n",
       " ('mr', 1153),\n",
       " ('his', 1145),\n",
       " ('.\"', 1138),\n",
       " ('at', 1031),\n",
       " (\"'\", 1007),\n",
       " ('so', 974),\n",
       " ('s', 935),\n",
       " ('emma', 865),\n",
       " ('all', 845),\n",
       " ('could', 837),\n",
       " ('would', 820),\n",
       " ('been', 759),\n",
       " ('him', 759),\n",
       " ('no', 742),\n",
       " ('my', 728),\n",
       " ('mrs', 699),\n",
       " ('on', 692),\n",
       " ('.--', 685),\n",
       " ('any', 654),\n",
       " ('do', 640),\n",
       " ('were', 600)]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from nltk.book import *\n",
    "fdist = FreqDist(emmawords)\n",
    "fdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5201"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "fdist['emma']\n",
    "fdist['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='ab'>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    " p = re.compile('ab*')\n",
    " p.match(\"abcd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def makeAlphaFreqDist(words):\n",
    "    adist = FreqDist()\n",
    "    pattern = re.compile('.*[^a-z].*')\n",
    "    for word in words:\n",
    "        if not pattern.match(word):\n",
    "            adist.update([word])\n",
    "    return adist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('to', 5239),\n",
       " ('the', 5201),\n",
       " ('and', 4896),\n",
       " ('of', 4291),\n",
       " ('i', 3178),\n",
       " ('a', 3129),\n",
       " ('it', 2528),\n",
       " ('her', 2469),\n",
       " ('was', 2398),\n",
       " ('she', 2340),\n",
       " ('in', 2188),\n",
       " ('not', 2140),\n",
       " ('you', 1980),\n",
       " ('be', 1975),\n",
       " ('that', 1806),\n",
       " ('he', 1806),\n",
       " ('had', 1624),\n",
       " ('but', 1441),\n",
       " ('as', 1436),\n",
       " ('for', 1347),\n",
       " ('have', 1320),\n",
       " ('is', 1240),\n",
       " ('with', 1217),\n",
       " ('very', 1202),\n",
       " ('mr', 1153),\n",
       " ('his', 1145),\n",
       " ('at', 1031),\n",
       " ('so', 974),\n",
       " ('s', 935),\n",
       " ('emma', 865),\n",
       " ('all', 845),\n",
       " ('could', 837),\n",
       " ('would', 820),\n",
       " ('been', 759),\n",
       " ('him', 759),\n",
       " ('no', 742),\n",
       " ('my', 728),\n",
       " ('mrs', 699),\n",
       " ('on', 692),\n",
       " ('any', 654),\n",
       " ('do', 640),\n",
       " ('were', 600),\n",
       " ('miss', 599),\n",
       " ('me', 573),\n",
       " ('by', 571),\n",
       " ('will', 570),\n",
       " ('must', 567),\n",
       " ('which', 556),\n",
       " ('there', 549),\n",
       " ('from', 546)]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "adist = makeAlphaFreqDist(emmawords)\n",
    "adist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "to 5239\nthe 5201\nand 4896\nof 4291\ni 3178\na 3129\nit 2528\nher 2469\nwas 2398\nshe 2340\nin 2188\nnot 2140\nyou 1980\nbe 1975\nthat 1806\nhe 1806\nhad 1624\nbut 1441\nas 1436\nfor 1347\nhave 1320\nis 1240\nwith 1217\nvery 1202\nmr 1153\nhis 1145\nat 1031\nso 974\ns 935\nemma 865\n"
     ]
    }
   ],
   "source": [
    "for word, freq in adist.most_common(30):\n",
    "    print(word, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(',', 18713),\n",
       " ('the', 13721),\n",
       " ('.', 6862),\n",
       " ('of', 6536),\n",
       " ('and', 6024),\n",
       " ('a', 4569),\n",
       " ('to', 4542),\n",
       " (';', 4072),\n",
       " ('in', 3916),\n",
       " ('that', 2982)]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "mbdist = FreqDist(text1)\n",
    "mbdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('to', 5239),\n",
       " ('the', 5201),\n",
       " ('and', 4896),\n",
       " ('of', 4291),\n",
       " ('i', 3178),\n",
       " ('a', 3129),\n",
       " ('it', 2528),\n",
       " ('her', 2469),\n",
       " ('was', 2398),\n",
       " ('she', 2340)]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "adist = makeAlphaFreqDist(emmawords)\n",
    "adist.most_common(10)"
   ]
  }
 ]
}