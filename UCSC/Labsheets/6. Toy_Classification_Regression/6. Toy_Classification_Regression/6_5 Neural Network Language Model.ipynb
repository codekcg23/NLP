{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:\n",
      "Train: 431408\n",
      "Dev: 124528\n",
      "Test: 128419\n",
      "﻿ __newline__ Project Gutenberg’s The Complete Works of William Shakespeare, by William __newline__ \n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "NEWLINE_TOKEN = ' __newline__ '\n",
    "UNK_TOKEN = '__unk__'\n",
    "\n",
    "# Read and collect text\n",
    "train_text = \"\"\n",
    "dev_text = \"\"\n",
    "test_text = \"\"\n",
    "texts = [train_text, dev_text, test_text]\n",
    "\n",
    "# Try the TED data set?\n",
    "for text_idx, file in enumerate(['./data/shakespeare/train.txt', './data/shakespeare/val.txt', './data/ted/test.txt']):\n",
    "    with open(file, 'r') as fp:\n",
    "        texts[text_idx] += NEWLINE_TOKEN.join([l.strip() for l in fp.readlines()]) + NEWLINE_TOKEN\n",
    "\n",
    "train_text, dev_text, test_text = texts\n",
    "\n",
    "print(\"Total characters:\")\n",
    "print(\"Train: %d\"%(len(train_text)))\n",
    "print(\"Dev: %d\"%(len(dev_text)))\n",
    "print(\"Test: %d\"%(len(test_text)))\n",
    "print(train_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text\n",
    "We usually preprocess the text to remove casing information, separate out punctuations etc to make our data cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens:\n",
      "Train: 81368\n",
      "Dev: 23839\n",
      "Test: 26162\n"
     ]
    }
   ],
   "source": [
    "tokens = [None, None, None]\n",
    "for text_idx in range(len(texts)):\n",
    "    tokens[text_idx] = word_tokenize(texts[text_idx].lower())\n",
    "\n",
    "train_tokens, dev_tokens, test_tokens = tokens\n",
    "\n",
    "print(\"Total tokens:\")\n",
    "print(\"Train: %d\"%(len(train_tokens)))\n",
    "print(\"Dev: %d\"%(len(dev_tokens)))\n",
    "print(\"Test: %d\"%(len(test_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6647\n",
      "Most frequent tokens\n",
      "\t__newline__: 10000\n",
      "\t,: 5218\n",
      "\t.: 4361\n",
      "\tthe: 1658\n",
      "\tand: 1456\n",
      "\ti: 1414\n",
      "\tto: 1254\n",
      "\t’: 1186\n",
      "\tof: 1111\n",
      "\tmy: 906\n",
      "Least frequent tokens\n",
      "\timpossible-: 1\n",
      "\tdescried: 1\n",
      "\tapproaching: 1\n",
      "\tfull-mann: 1\n",
      "\tsixty: 1\n",
      "\tsecurity: 1\n",
      "\tassurance: 1\n",
      "\tforgo: 1\n",
      "\trenowned: 1\n",
      "\tunexecuted: 1\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "full_vocab = dict()\n",
    "for token in train_tokens:\n",
    "    full_vocab[token] = full_vocab.get(token, 0) + 1\n",
    "\n",
    "# Sort vocabulary by occurence\n",
    "sorted_vocab = sorted(full_vocab.keys(), key=lambda word: -full_vocab[word])\n",
    "\n",
    "# Print some samples\n",
    "print(\"Vocabulary size: %d\"%(len(sorted_vocab)))\n",
    "print(\"Most frequent tokens\")\n",
    "for i in range(10):\n",
    "    print(\"\\t%s: %d\"%(sorted_vocab[i], full_vocab[sorted_vocab[i]]))\n",
    "print(\"Least frequent tokens\")\n",
    "for i in range(1,11):\n",
    "    print(\"\\t%s: %d\"%(sorted_vocab[-i], full_vocab[sorted_vocab[-i]]))\n",
    "\n",
    "# Create final vocab\n",
    "word2idx = {w: idx for idx, w in enumerate(sorted_vocab[:VOCAB_SIZE])}\n",
    "idx2word = {idx: w for idx, w in enumerate(sorted_vocab[:VOCAB_SIZE])}\n",
    "\n",
    "word2idx[UNK_TOKEN] = VOCAB_SIZE # The last element is the UNK token\n",
    "idx2word[VOCAB_SIZE] = UNK_TOKEN\n",
    "VOCAB_SIZE = VOCAB_SIZE + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter text based on vocabulary\n",
    "We will now have to replace words we do not have in the vocabulary with a special token, `__unk__` in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens filtered out as unknown:\n",
      "Train: 1647/81368\n",
      "Dev: 1938/23839\n",
      "Test: 5162/26162\n"
     ]
    }
   ],
   "source": [
    "for tokens_idx in range(len(tokens)):\n",
    "    tokens[tokens_idx] = [t if t in word2idx else UNK_TOKEN for t in tokens[tokens_idx]]\n",
    "\n",
    "train_tokens, dev_tokens, test_tokens = tokens\n",
    "print(\"Number of tokens filtered out as unknown:\")\n",
    "print(\"Train: %d/%d\"%(len([1 for t in train_tokens if t == UNK_TOKEN]), len(train_tokens)))\n",
    "print(\"Dev: %d/%d\"%(len([1 for t in dev_tokens if t == UNK_TOKEN]), len(dev_tokens)))\n",
    "print(\"Test: %d/%d\"%(len([1 for t in test_tokens if t == UNK_TOKEN]), len(test_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data in tensor form\n",
    "Our keras models finally take tensors as input and labels, so we need to modify our data to fit this form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([word2idx[t] for t in train_tokens]) # Make lists of indexes corresponding to words in each sentence and create an array of all sentences\n",
    "X_dev = np.array([word2idx[t] for t in dev_tokens])\n",
    "X_test = np.array([word2idx[t] for t in test_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labels in this exercise are just the next words. Hence, for\n",
    "\n",
    ">   `X_train = ['hello', 'how', 'are', 'you', '?']`\n",
    "\n",
    "we will have:\n",
    "\n",
    ">    `y_train = ['how, 'are', you', '?']`\n",
    "\n",
    "Which is just `X_train[1:]`\n",
    "We will also remove the last element of `X_train`, since we do not have any label for it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bag_of_words(X, context_size=1, vocab_size=VOCAB_SIZE):\n",
    "    num_examples = X.shape[0]-context_size  # There's no next word for the last word!\n",
    "    X_bow = np.zeros((num_examples, vocab_size)) # Initialize the vector\n",
    "    \n",
    "    y_bow = np.zeros((num_examples, vocab_size))\n",
    "    \n",
    "    for idx in range(num_examples):\n",
    "        for context_idx in range(context_size):\n",
    "            X_bow[idx, X[idx+context_idx]] = 1\n",
    "        y_bow[idx, X[idx + context_size]] = 1\n",
    "    \n",
    "    return X_bow, y_bow\n",
    "            \n",
    "def get_next_predicted_word(model, input_words, context_size=1):\n",
    "    if not isinstance(input_words, list):\n",
    "        input_words = [input_words]\n",
    "    input_words = input_words + [\"__unk__\"]\n",
    "    input_array = np.array([word2idx[w] for w in input_words])\n",
    "    input_bow, _ = build_bag_of_words(input_array, context_size=context_size)\n",
    "    scores = model.predict(input_bow)\n",
    "    output_word = idx2word[np.argmax(scores)]\n",
    "    \n",
    "    return output_word\n",
    "\n",
    "def get_sentence(model, start_words, context_size=1):\n",
    "    if not isinstance(start_words, list):\n",
    "        start_words = [start_words]\n",
    "\n",
    "    output = [] + start_words\n",
    "    while output[-1] != '__newline__' and len(output) < 100:\n",
    "        prev_word = get_next_predicted_word(model, output[-context_size:], context_size=context_size)\n",
    "        output.append(prev_word)\n",
    "    return \" \".join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unigram, y_train_unigram = build_bag_of_words(X_train, context_size=1)\n",
    "X_dev_unigram, y_dev_unigram = build_bag_of_words(X_dev, context_size=1)\n",
    "X_test_unigram, y_test_unigram = build_bag_of_words(X_test, context_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81367, 5001)\n",
      "(23838, 5001)\n",
      "(26161, 5001)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_unigram.shape)\n",
    "print(X_dev_unigram.shape)\n",
    "print(X_test_unigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               500200    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5001)              505101    \n",
      "=================================================================\n",
      "Total params: 1,025,501\n",
      "Trainable params: 1,025,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(VOCAB_SIZE,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(VOCAB_SIZE, activation='softmax')) # Equivalent to adding a softmax layer\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/arw/anaconda3/envs/text-analytics/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 81367 samples, validate on 23838 samples\n",
      "Epoch 1/1\n",
      "81367/81367 [==============================] - 44s 546us/step - loss: 5.6463 - acc: 0.1485 - val_loss: 5.1174 - val_acc: 0.1540\n",
      "i , __newline__\n",
      "Train on 81367 samples, validate on 23838 samples\n",
      "Epoch 2/2\n",
      "13312/81367 [===>..........................] - ETA: 34s - loss: 5.1738 - acc: 0.1828"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4605bafc6fe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_unigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_unigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev_unigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev_unigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/text-analytics/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/text-analytics/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    184\u001b[0m                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n\u001b[1;32m    185\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[0;32m~/anaconda3/envs/text-analytics/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/text-analytics/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.fit(X_train_unigram, y_train_unigram, batch_size=128, epochs=epoch+1, initial_epoch=epoch, validation_data=(X_dev_unigram, y_dev_unigram))\n",
    "    print(get_sentence(model, ['i']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "think __newline__\n",
      "well . __newline__\n",
      "i __unk__ , __newline__\n",
      "who , __newline__\n"
     ]
    }
   ],
   "source": [
    "print(get_sentence(model, ['think']))\n",
    "print(get_sentence(model, ['well']))\n",
    "print(get_sentence(model, ['i']))\n",
    "print(get_sentence(model, ['who']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add context\n",
    "The above data uses only _one_ previous word as context, but we can change our data to include more words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bigram, y_train_bigram = build_bag_of_words(X_train, context_size=2)\n",
    "X_dev_bigram, y_dev_bigram = build_bag_of_words(X_dev, context_size=2)\n",
    "X_test_bigram, y_test_bigram = build_bag_of_words(X_test, context_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81366, 5001)\n",
      "(81366, 5001)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_bigram.shape)\n",
    "print(y_train_bigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 100)               500200    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5001)              505101    \n",
      "=================================================================\n",
      "Total params: 1,025,501\n",
      "Trainable params: 1,025,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bigram = Sequential()\n",
    "model_bigram.add(Dense(100, input_shape=(VOCAB_SIZE,)))\n",
    "model_bigram.add(Dense(100, activation='relu'))\n",
    "model_bigram.add(Dense(100, activation='relu'))\n",
    "model_bigram.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
    "\n",
    "model_bigram.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_bigram.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 81366 samples, validate on 23837 samples\n",
      "Epoch 1/1\n",
      "32768/81366 [===========>..................] - ETA: 27s - loss: 6.0796 - acc: 0.1184"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ca4097c15951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel_bigram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_bigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_bigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_dev_bigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dev_bigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_bigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'have'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/text-analytics/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/text-analytics/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    184\u001b[0m                             fit_inputs[:-1], batch_ids) + [fit_inputs[-1]]\n\u001b[1;32m    185\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[0;32m~/anaconda3/envs/text-analytics/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/text-analytics/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model_bigram.fit(X_train_bigram, y_train_bigram, batch_size=128, epochs=epoch+1, initial_epoch=epoch, validation_data=(X_dev_bigram, y_dev_bigram))\n",
    "    print(get_sentence(model_bigram, ['i', 'have'], context_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "think of doting 5 mourn mourn officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer\n",
      "well we perish require officer perish officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer\n",
      "i have looks college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college college\n",
      "who will shady silver clay officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer kind-hearted officer officer\n"
     ]
    }
   ],
   "source": [
    "print(get_sentence(model_bigram, ['think', 'of'], context_size=2))\n",
    "print(get_sentence(model_bigram, ['well', 'we'], context_size=2))\n",
    "print(get_sentence(model_bigram, ['i', 'have'], context_size=2))\n",
    "print(get_sentence(model_bigram, ['who', 'will'], context_size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_trigram, y_train_trigram = build_bag_of_words(X_train, context_size=3)\n",
    "X_dev_trigram, y_dev_trigram = build_bag_of_words(X_dev, context_size=3)\n",
    "X_test_trigram, y_test_trigram = build_bag_of_words(X_test, context_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(234077, 5001)\n",
      "(234077, 5001)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_trigram.shape)\n",
    "print(y_train_trigram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 100)               500200    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 5001)              505101    \n",
      "=================================================================\n",
      "Total params: 1,025,501\n",
      "Trainable params: 1,025,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_trigram = Sequential()\n",
    "model_trigram.add(Dense(100, input_shape=(VOCAB_SIZE,)))\n",
    "model_trigram.add(Dense(100, activation='relu'))\n",
    "model_trigram.add(Dense(100, activation='relu'))\n",
    "model_trigram.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
    "\n",
    "model_trigram.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model_trigram.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 1/1\n",
      "234077/234077 [==============================] - 128s 546us/step - loss: 5.1627 - acc: 0.1738 - val_loss: 4.5636 - val_acc: 0.2223\n",
      "i have to be the __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 2/2\n",
      "234077/234077 [==============================] - 132s 566us/step - loss: 4.6285 - acc: 0.2311 - val_loss: 4.4308 - val_acc: 0.2419\n",
      "i have to be a __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 3/3\n",
      "234077/234077 [==============================] - 134s 571us/step - loss: 4.4008 - acc: 0.2486 - val_loss: 4.4002 - val_acc: 0.2478\n",
      "i have to __unk__ the __unk__ of __unk__ , and the __unk__ __unk__ of , and course i & apos ; s a __unk__ , __unk__ and the __unk__ __unk__ of , and course i & apos ; s a __unk__ , __unk__ and the __unk__ __unk__ of , and course i & apos ; s a __unk__ , __unk__ and the __unk__ __unk__ of , and course i & apos ; s a __unk__ , __unk__ and the __unk__ __unk__ of , and course i & apos ; s a __unk__ , __unk__ and the __unk__ __unk__ of ,\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 4/4\n",
      "234077/234077 [==============================] - 134s 570us/step - loss: 4.2256 - acc: 0.2597 - val_loss: 4.4195 - val_acc: 0.2503\n",
      "i have to be a __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 5/5\n",
      "234077/234077 [==============================] - 133s 567us/step - loss: 4.0682 - acc: 0.2675 - val_loss: 4.4739 - val_acc: 0.2511\n",
      "i have to __unk__ __unk__ the , __unk__ __unk__ and __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 6/6\n",
      "234077/234077 [==============================] - 137s 584us/step - loss: 3.9213 - acc: 0.2756 - val_loss: 4.5877 - val_acc: 0.2504\n",
      "i have to __unk__ __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 7/7\n",
      "234077/234077 [==============================] - 136s 580us/step - loss: 3.7823 - acc: 0.2840 - val_loss: 4.7392 - val_acc: 0.2505\n",
      "i have to __unk__ __unk__ the of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 8/8\n",
      "234077/234077 [==============================] - 136s 583us/step - loss: 3.6531 - acc: 0.2915 - val_loss: 4.9070 - val_acc: 0.2496\n",
      "i have to __unk__ __unk__ the of __unk__ __unk__ __unk__ , and i was __unk__ , __unk__ and i that & apos ; s a __unk__ of __unk__ __unk__ __unk__ , and i was __unk__ , __unk__ and i that & apos ; s a __unk__ of __unk__ __unk__ __unk__ , and i was __unk__ , __unk__ and i that & apos ; s a __unk__ of __unk__ __unk__ __unk__ , and i was __unk__ , __unk__ and i that & apos ; s a __unk__ of __unk__ __unk__ __unk__ , and i was __unk__ , __unk__ and i that\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 9/9\n",
      "234077/234077 [==============================] - 136s 582us/step - loss: 3.5330 - acc: 0.2998 - val_loss: 5.1068 - val_acc: 0.2457\n",
      "i have to __unk__ __unk__ the of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 10/10\n",
      "234077/234077 [==============================] - 137s 586us/step - loss: 3.4259 - acc: 0.3092 - val_loss: 5.3306 - val_acc: 0.2452\n",
      "i have to __unk__ __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 11/11\n",
      "234077/234077 [==============================] - 136s 583us/step - loss: 3.3299 - acc: 0.3185 - val_loss: 5.5235 - val_acc: 0.2446\n",
      "i have to do . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 12/12\n",
      "234077/234077 [==============================] - 136s 580us/step - loss: 3.2444 - acc: 0.3289 - val_loss: 5.6936 - val_acc: 0.2404\n",
      "i have to do . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 13/13\n",
      "234077/234077 [==============================] - 136s 580us/step - loss: 3.1696 - acc: 0.3392 - val_loss: 5.9064 - val_acc: 0.2369\n",
      "i have to do . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 14/14\n",
      "234077/234077 [==============================] - 137s 584us/step - loss: 3.1025 - acc: 0.3469 - val_loss: 6.0596 - val_acc: 0.2363\n",
      "i have to __unk__ __unk__ the of the united states , belshazzar , , , but the __unk__ of the the united states , belshazzar , , , but the __unk__ of the the united states , belshazzar , , , but the __unk__ of the the united states , belshazzar , , , but the __unk__ of the the united states , belshazzar , , , but the __unk__ of the the united states , belshazzar , , , but the __unk__ of the the united states , belshazzar , , , but the __unk__ of the the united\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 15/15\n",
      "234077/234077 [==============================] - 137s 584us/step - loss: 3.0416 - acc: 0.3551 - val_loss: 6.1924 - val_acc: 0.2327\n",
      "i have to do . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 16/16\n",
      "234077/234077 [==============================] - 136s 581us/step - loss: 2.9871 - acc: 0.3644 - val_loss: 6.3445 - val_acc: 0.2356\n",
      "i have to admit that , of we & apos ; s a __unk__ premise that is a __unk__ __unk__ of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 17/17\n",
      "234077/234077 [==============================] - 136s 582us/step - loss: 2.9383 - acc: 0.3714 - val_loss: 6.5133 - val_acc: 0.2321\n",
      "i have to __unk__ address the , u.s. and __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 18/18\n",
      "234077/234077 [==============================] - 135s 576us/step - loss: 2.8935 - acc: 0.3783 - val_loss: 6.7054 - val_acc: 0.2304\n",
      "i have to admit that , of course , , it & apos ; s a __unk__ in __unk__ the of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 19/19\n",
      "234077/234077 [==============================] - 137s 585us/step - loss: 2.8522 - acc: 0.3841 - val_loss: 6.8180 - val_acc: 0.2314\n",
      "i have to say a noble goodbye , a lot of people are deeply __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 20/20\n",
      "234077/234077 [==============================] - 136s 581us/step - loss: 2.8156 - acc: 0.3906 - val_loss: 6.9559 - val_acc: 0.2291\n",
      "i have to do with the iranians . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 21/21\n",
      "234077/234077 [==============================] - 136s 580us/step - loss: 2.7795 - acc: 0.3961 - val_loss: 7.0966 - val_acc: 0.2279\n",
      "i have to __unk__ a __unk__ , and i & apos ; s a __unk__ premise that says , & quot ; __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 22/22\n",
      "234077/234077 [==============================] - 133s 570us/step - loss: 2.7486 - acc: 0.4023 - val_loss: 7.2838 - val_acc: 0.2268\n",
      "i have to do . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 23/23\n",
      "234077/234077 [==============================] - 137s 584us/step - loss: 2.7184 - acc: 0.4065 - val_loss: 7.3507 - val_acc: 0.2256\n",
      "i have to __unk__ to the __unk__ __unk__ of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 24/24\n",
      "234077/234077 [==============================] - 137s 584us/step - loss: 2.6918 - acc: 0.4114 - val_loss: 7.4702 - val_acc: 0.2239\n",
      "i have to __unk__ a __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 25/25\n",
      "234077/234077 [==============================] - 131s 561us/step - loss: 2.6653 - acc: 0.4152 - val_loss: 7.5581 - val_acc: 0.2230\n",
      "i have to admit that we exist can __unk__ in __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the of the __unk__ the\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 26/26\n",
      "234077/234077 [==============================] - 128s 546us/step - loss: 2.6418 - acc: 0.4196 - val_loss: 7.6382 - val_acc: 0.2256\n",
      "i have to say to that , be sounds __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 27/27\n",
      "234077/234077 [==============================] - 128s 545us/step - loss: 2.6199 - acc: 0.4229 - val_loss: 7.7873 - val_acc: 0.2248\n",
      "i have to admit that we exist __unk__ from the __unk__ __unk__ of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 28/28\n",
      "234077/234077 [==============================] - 134s 571us/step - loss: 2.5988 - acc: 0.4269 - val_loss: 7.9261 - val_acc: 0.2246\n",
      "i have to do with you something . & quot ; __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 29/29\n",
      "234077/234077 [==============================] - 137s 587us/step - loss: 2.5795 - acc: 0.4303 - val_loss: 8.0188 - val_acc: 0.2235\n",
      "i have to __unk__ to the __unk__ __unk__ of __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__ __unk__ __unk__ in the , __unk__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 30/30\n",
      "234077/234077 [==============================] - 137s 587us/step - loss: 2.5617 - acc: 0.4335 - val_loss: 8.1486 - val_acc: 0.2244\n",
      "i have to __unk__ to the __unk__ __unk__ of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 31/31\n",
      "234077/234077 [==============================] - 133s 568us/step - loss: 2.5433 - acc: 0.4371 - val_loss: 8.1420 - val_acc: 0.2193\n",
      "i have to admit that there are exceptions here does not be . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 32/32\n",
      "234077/234077 [==============================] - 135s 577us/step - loss: 2.5282 - acc: 0.4394 - val_loss: 8.3211 - val_acc: 0.2203\n",
      "i have to admit that . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 33/33\n",
      "234077/234077 [==============================] - 136s 582us/step - loss: 2.5119 - acc: 0.4416 - val_loss: 8.4854 - val_acc: 0.2220\n",
      "i have to admit that . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 34/34\n",
      "234077/234077 [==============================] - 137s 583us/step - loss: 2.4976 - acc: 0.4448 - val_loss: 8.5046 - val_acc: 0.2226\n",
      "i have to start answering . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 35/35\n",
      "234077/234077 [==============================] - 137s 584us/step - loss: 2.4850 - acc: 0.4469 - val_loss: 8.5823 - val_acc: 0.2192\n",
      "i have to say to , & quot ; __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 36/36\n",
      "234077/234077 [==============================] - 135s 579us/step - loss: 2.4713 - acc: 0.4494 - val_loss: 8.7226 - val_acc: 0.2209\n",
      "i have to admit that there are a few times . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 37/37\n",
      "234077/234077 [==============================] - 136s 583us/step - loss: 2.4595 - acc: 0.4521 - val_loss: 8.6879 - val_acc: 0.2204\n",
      "i have to admit that there are a quarter of million them , which is a ring road around london , recently , we & apos ; s a __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 38/38\n",
      "234077/234077 [==============================] - 135s 576us/step - loss: 2.4472 - acc: 0.4530 - val_loss: 8.8836 - val_acc: 0.2209\n",
      "i have to admit that . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 39/39\n",
      "234077/234077 [==============================] - 136s 582us/step - loss: 2.4366 - acc: 0.4547 - val_loss: 8.9757 - val_acc: 0.2194\n",
      "i have to say to , & quot ; __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 40/40\n",
      "234077/234077 [==============================] - 137s 586us/step - loss: 2.4263 - acc: 0.4564 - val_loss: 8.9936 - val_acc: 0.2182\n",
      "i have to say to myself , and i & apos ; s a __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 41/41\n",
      "234077/234077 [==============================] - 137s 585us/step - loss: 2.4154 - acc: 0.4587 - val_loss: 9.1049 - val_acc: 0.2195\n",
      "i have to do it , and & quot ; __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 42/42\n",
      "234077/234077 [==============================] - 136s 581us/step - loss: 2.4054 - acc: 0.4605 - val_loss: 9.1707 - val_acc: 0.2184\n",
      "i have to __unk__ to the __unk__ __unk__ of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 43/43\n",
      "234077/234077 [==============================] - 132s 566us/step - loss: 2.3955 - acc: 0.4616 - val_loss: 9.2490 - val_acc: 0.2194\n",
      "i have to __unk__ come out , and i & apos ; s a __unk__ of __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 44/44\n",
      "234077/234077 [==============================] - 136s 583us/step - loss: 2.3867 - acc: 0.4636 - val_loss: 9.3117 - val_acc: 0.2176\n",
      "i have to do it . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 45/45\n",
      "234077/234077 [==============================] - 134s 573us/step - loss: 2.3788 - acc: 0.4643 - val_loss: 9.3428 - val_acc: 0.2182\n",
      "i have to admit that we exist __unk__ from the textbook , it looked like . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 46/46\n",
      "234077/234077 [==============================] - 136s 583us/step - loss: 2.3707 - acc: 0.4664 - val_loss: 9.4899 - val_acc: 0.2161\n",
      "i have to admit that we are inevitably going to __unk__ the __unk__ of __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 47/47\n",
      "234077/234077 [==============================] - 137s 587us/step - loss: 2.3622 - acc: 0.4672 - val_loss: 9.5189 - val_acc: 0.2170\n",
      "i have to admit that we exist __unk__ from , __unk__ __unk__ __unk__ . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 48/48\n",
      "234077/234077 [==============================] - 137s 584us/step - loss: 2.3552 - acc: 0.4689 - val_loss: 9.6049 - val_acc: 0.2177\n",
      "i have to __unk__ tradition change , performance and raised truth is systems -- evidence which talent if you can find a tumor in the upper right quadrant , where performance is strong and learning opportunities are equally distributed . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 49/49\n",
      "234077/234077 [==============================] - 136s 582us/step - loss: 2.3469 - acc: 0.4709 - val_loss: 9.6810 - val_acc: 0.2164\n",
      "i have to admit that . __newline__\n",
      "Train on 234077 samples, validate on 60283 samples\n",
      "Epoch 50/50\n",
      "234077/234077 [==============================] - 136s 583us/step - loss: 2.3396 - acc: 0.4714 - val_loss: 9.7029 - val_acc: 0.2153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have to admit that we exist __unk__ from , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__ __unk__ , __unk__ __unk__\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model_trigram.fit(X_train_trigram, y_train_trigram, batch_size=128, epochs=epoch+1, initial_epoch=epoch, validation_data=(X_dev_trigram, y_dev_trigram))\n",
    "    print(get_sentence(model_trigram, ['i', 'have','to'], context_size=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
