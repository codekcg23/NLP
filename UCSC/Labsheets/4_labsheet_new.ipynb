{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "fbbb7d2143a1d68e1cf272edf0974e702b621cb99b4ee39ce84db3bf0ffb588e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Lab Sheet 4: Stemming and Lemmatization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Stemming and Lemmatizatio"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "emma tokens - ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'\", 's', 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.', 'Her']\nemma words-  ['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'\", 's', 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.', 'her']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "file0=nltk.corpus.gutenberg.fileids()[0]\n",
    "emmatext=nltk.corpus.gutenberg.raw(file0)\n",
    "emmatokens=nltk.wordpunct_tokenize(emmatext)\n",
    "print(\"emma tokens -\", emmatokens[:100])\n",
    "emmawords=[w.lower() for w in emmatokens]\n",
    "print(\"emma words- \",emmawords[:100])"
   ]
  },
  {
   "source": [
    "#### Creating stemmers in nltk"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating stemmers in nltk\n",
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "source": [
    "#### Porter Stemmer work with regular case grammar"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'I', 'chapter', 'I', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfort', 'home', 'and', 'happi', 'disposit', ',', 'seem', 'to', 'unit', 'some', 'of', 'the', 'best', 'bless', 'of', 'exist', ';', 'and', 'had', 'live', 'nearli', 'twenti', '-', 'one', 'year', 'in', 'the']\n",
      "['emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'i', 'chapter', 'i', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfort', 'home', 'and', 'happi', 'disposit', ',', 'seem', 'to', 'unit', 'some', 'of', 'the', 'best', 'bless', 'of', 'exist', ';', 'and', 'had', 'live', 'nearli', 'twenti', '-', 'one', 'year', 'in', 'the']\n"
     ]
    }
   ],
   "source": [
    "emmaRegStem = [porter.stem(t) for t in emmatokens]\n",
    "print(emmaRegStem[1:50])\n",
    "emmaLowerStem = [porter.stem(w) for w in emmawords]\n",
    "print(emmaLowerStem[1:50])"
   ]
  },
  {
   "source": [
    "#### Lancaster Stemmer work with regular case grammar"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['emm', 'by', 'jan', 'aust', '1816', ']', 'volum', 'i', 'chapt', 'i', 'emm', 'woodh', ',', 'handsom', ',', 'clev', ',', 'and', 'rich', ',', 'with', 'a', 'comfort', 'hom', 'and', 'happy', 'disposit', ',', 'seem', 'to', 'unit', 'som', 'of', 'the', 'best', 'bless', 'of', 'ex', ';', 'and', 'had', 'liv', 'near', 'twenty', '-', 'on', 'year', 'in', 'the']\n",
      "['emm', 'by', 'jan', 'aust', '1816', ']', 'volum', 'i', 'chapt', 'i', 'emm', 'woodh', ',', 'handsom', ',', 'clev', ',', 'and', 'rich', ',', 'with', 'a', 'comfort', 'hom', 'and', 'happy', 'disposit', ',', 'seem', 'to', 'unit', 'som', 'of', 'the', 'best', 'bless', 'of', 'ex', ';', 'and', 'had', 'liv', 'near', 'twenty', '-', 'on', 'year', 'in', 'the']\n"
     ]
    }
   ],
   "source": [
    "# stemmer work with regular case grammar for lancaster stemmer\n",
    "emmaRegStem = [lancaster.stem(t) for t in emmatokens]\n",
    "print(emmaRegStem[1:50])\n",
    "emmaLowerStem = [lancaster.stem(w) for w in emmawords]\n",
    "print(emmaLowerStem[1:50])"
   ]
  },
  {
   "source": [
    "#### Creating our own Stemmer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Cry'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "def stem(word):\n",
    "    for suffix in ['ing','ly','ed','ious','ies','ive','es','s']:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "stemmedWord = stem(\"Crying\")\n",
    "stemmedWord"
   ]
  },
  {
   "source": [
    "#### Lemmatization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessing', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty', '-', 'one', 'year', 'in', 'the']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization - Gives existing words unlike stemmer\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "emmaLemma = [wnl.lemmatize(t) for t in emmawords]\n",
    "print(emmaLemma[1:50])\n"
   ]
  },
  {
   "source": [
    "#### Own word punctuation and Tokenizer with regex"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Emma',\n",
       " 'by',\n",
       " 'Jane',\n",
       " 'Austen',\n",
       " '1816',\n",
       " 'VOLUME',\n",
       " 'I',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " 'Emma',\n",
       " 'Woodhouse',\n",
       " 'handsome',\n",
       " 'clever',\n",
       " 'and',\n",
       " 'rich',\n",
       " 'with',\n",
       " 'a',\n",
       " 'comfortable',\n",
       " 'home',\n",
       " 'and',\n",
       " 'happy',\n",
       " 'disposition',\n",
       " 'seemed',\n",
       " 'to']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# we can create our own word puncutation and tokenizors\n",
    "# re.match() -> any match at the beginng of stirng\n",
    "# re.search() -> anywhere in String\n",
    "# re.findall() -> substrings in anywhere of the string\n",
    "shorttext = emmatext[:150]\n",
    "import re\n",
    "pWord = re.compile(('\\w+'))\n",
    "re.findall(pWord,shorttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['U', 'S', 'A', 'poster', 'print', 'costs', '12', '40', 'with', '10', 'off']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "specialtext='U.S.A. poster-print costs $12.40 with 10% off.'\n",
    "re.findall(pWord,specialtext)"
   ]
  },
  {
   "source": [
    "#### Matching inner hypthen"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('U', ''),\n",
       " ('S', ''),\n",
       " ('A', ''),\n",
       " ('poster-print', '-print'),\n",
       " ('costs', ''),\n",
       " ('12', ''),\n",
       " ('40', ''),\n",
       " ('with', ''),\n",
       " ('10', ''),\n",
       " ('off', '')]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "pToken=re.compile('(\\w+(-\\w+)*)')\n",
    "re.findall(pToken,specialtext)"
   ]
  },
  {
   "source": [
    "#### Matching Abbreviations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('U.S.A.', 'A.')]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# matching Abbrevations\n",
    "pAbbrev=re.compile('(([A-Z]\\.)+)')\n",
    "re.findall(pAbbrev,specialtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('U', '', ''),\n",
       " ('S', '', ''),\n",
       " ('A', '', ''),\n",
       " ('poster-print', '-print', ''),\n",
       " ('costs', '', ''),\n",
       " ('12', '', ''),\n",
       " ('40', '', ''),\n",
       " ('with', '', ''),\n",
       " ('10', '', ''),\n",
       " ('off', '', '')]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "pToken=re.compile('(\\w+(-\\w+)*|([A-Z]\\.)+)')\n",
    "re.findall(pToken,specialtext)"
   ]
  },
  {
   "source": [
    "#### Order is important"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('U.S.A.', 'A.', ''),\n",
       " ('poster-print', '', '-print'),\n",
       " ('costs', '', ''),\n",
       " ('12', '', ''),\n",
       " ('40', '', ''),\n",
       " ('with', '', ''),\n",
       " ('10', '', ''),\n",
       " ('off', '', '')]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "pToken=re.compile('(([A-Z]\\.)+|\\w+(-\\w+)*)')\n",
    "re.findall(pToken,specialtext)"
   ]
  },
  {
   "source": [
    "#### Matching currency"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('U.S.A.', 'A.', '', ''),\n",
       " ('poster-print', '', '-print', ''),\n",
       " ('costs', '', '', ''),\n",
       " ('$12.40', '', '', '.40'),\n",
       " ('with', '', '', ''),\n",
       " ('10', '', '', ''),\n",
       " ('off', '', '', '')]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Matching currency\n",
    "pToken=re.compile(r'(([A-Z]\\.)+|\\w+(-\\w+)*|\\$?\\d+(\\.\\d+)?)')\n",
    "\n",
    "re.findall(pToken,specialtext)\n",
    "# r -> acceptes \\ as string"
   ]
  },
  {
   "source": [
    "#### Clearly write patterns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pToken = re.compile(r'''([A-Z]\\.)+    #abbreviations,e.g.U.S.A.\n",
    "        | \\w+(-\\w+)*                  #words with internal hyphens\n",
    "        | \\$?\\d+(\\.\\d+)?              #currency,like$12.40\n",
    "        ''', re.X)                    #verboseflag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built in NLTK tokenizer remove need of paranthessis\n",
    "pattern=r'''(?x)                #set flag to verbose regexp\n",
    "           (?:[A-Z]\\.)+         # abbreiviations\n",
    "          | \\w+(?:-\\w+)*        # words with internal hypthens\n",
    "          | \\$?\\d+(?:\\.\\d+)?%?  #currency and percentages\n",
    "          | \\.\\.\\.              #ellipsis\n",
    "          | [][.,;\"'?():-_']    #separate special character token\n",
    "          '''\n",
    "# Gave an error when using non capturing parenthesis\n",
    "# https://stackoverflow.com/questions/36353125/nltk-regular-expression-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['U.S.A.', 'poster-print', 'costs', '$12.40', 'with', '10', 'off', '.']"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "nltk.regexp_tokenize(shorttext,pattern)\n",
    "nltk.regexp_tokenize(specialtext,pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetPattern= r'''(?x)         #set flag to allow verbose regexps\n",
    " (https?://|www)\\S+         #simpleURLs\n",
    "| (:-\\)|;-\\))                #small list of emoticons\n",
    "| &(amp|lt|gt|quot);         # XML or HTML entity\n",
    "| \\#\\w+                      # hash tags\n",
    "| @\\w+                       # mentions\n",
    "| \\d+:\\d+                    #t ime like pattern\n",
    "| \\d+\\.\\d+                   # number with a decimal\n",
    "| (?:\\d+,)+?\\d{3}(?=([^,]|$))  # number with a comma\n",
    "| (?:[A-Z]\\.)+                 # simple abbreviations\n",
    "| (--+)                      # multiple dashes\n",
    "| \\w+(?:-\\w+)*                 #words with internal hyphens or apostrophes\n",
    "| ['\\\".?!,:;]+               # special characters\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1=\"@natalieohayre I agree #hc09 needs reform- but not by crooked politicians who r clueless about healthcare! #tcot #fishy NO GOV'T TAKEOVER!\"\n",
    "\n",
    "tweet2 = \"To Sen. Roland Burris: Affordable, quality health insurance can't wait http://bit.ly/j63je #hc09 #IL #60660\"\n",
    "\n",
    "tweet3 = \"RT @karoli: RT @Seriou: .@whitehouse I will stand w/ Obama on #healthcare, I trust him. #p2 #tlot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('http://', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('', '', '', '', ''),\n",
       " ('', '', '', '', '')]"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "nltk.regexp_tokenize(tweet2,tweetPattern)\n",
    "## doens't work this one and also I was unable to debug it"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}